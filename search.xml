<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[鼠标操作初级]]></title>
    <url>%2F2019%2F03%2F17%2FOpenCV-Learning-Day-7%2F</url>
    <content type="text"><![CDATA[前言：因为今天天气不错，所以没有前言。 测试代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;#define WINDOW_NAME "SetMouseCallback Sample"void on_MouseHandle(int event,int x,int y,int flags,void* param);void DrawRectangle(Mat&amp; img,Rect box);void ShowHelpText();Rect g_rectangle;bool g_bDrawingBox = false;RNG g_rng(12345);int main(int argc,char** argv)&#123;g_rectangle = Rect(-1,-1,0,0);Mat srcImage(600,800,CV_8UC3),tempImage;srcImage.copyTo(tempImage);srcImage = Scalar::all(0);namedWindow(WINDOW_NAME);setMouseCallback(WINDOW_NAME,on_MouseHandle,(void*)&amp;srcImage);while(1)&#123;srcImage.copyTo(tempImage);if(g_bDrawingBox) DrawRectangle(tempImage, g_rectangle);imshow(WINDOW_NAME,tempImage);if(waitKey(10) == 27) break;&#125;return 0;&#125;void on_MouseHandle(int event,int x,int y,int flags,void* param)&#123;Mat&amp; image = *(Mat*) param;switch (event)&#123;//鼠标移动消息case EVENT_MOUSEMOVE:&#123;if (g_bDrawingBox) &#123;g_rectangle.width = x - g_rectangle.x;g_rectangle.height = y - g_rectangle.y;&#125;&#125;break;//左键按下消息case EVENT_LBUTTONDOWN:&#123;g_bDrawingBox = true;g_rectangle = Rect(x,y,0,0);&#125;break;//左键抬起消息case EVENT_LBUTTONUP:&#123;g_bDrawingBox = false;//对宽和高小于0的处理if (g_rectangle.width &lt; 0) &#123;g_rectangle.x += g_rectangle.width;g_rectangle.width *= -1;&#125;DrawRectangle(image,g_rectangle);&#125;break;&#125;&#125;void DrawRectangle(Mat&amp; img,Rect box)&#123;rectangle(img,box.tl(),box.br(),Scalar(g_rng.uniform(0,255),g_rng.uniform(0,255),g_rng.uniform(0,255)));&#125;效果图：setMouseCallback()函数函数原型：1234void setMouseCallback(const String &amp;winname, MouseCallback onMouse, void *userdata = (void *)0)第一个参数为窗口名称第二个参数为MouseCallback类型的onMouse，窗口里每次鼠标操作的发生都会调用这个函数，这个函数的原型：123456void on_MouseHandle(int event, int x, int y, int flags, void *param)event是许多EVENT_变量，这个在测试程序中有所体现；x和y是在图像坐标系（？）的坐标值，flags是EVENT_FLAG的组合，param是用户定义的传递到setMouseCallback()函数调用的参数。第三个参数是用户定义的传递到回调函数的参数，有默认值0 一、Scalar()函数：函数定义：1234typedef struct Scalar&#123;double val[4];&#125;Scalar;由函数定义可知Scalar()可以存储4个double类型的值，分别对应4通道（需要注意的是，在OpenCV中，前三个颜色通道分别是BGR而不是常识中的RGB，第四个通道则是透明度Alpha值），在使用Scalar()函数的过程中，具体需要使用几个值需要看Mat类型图像的type，比如常见的CV_8UC3类型，C就代表通道（channel），看到C3就知道这个图像有三个通道，因此应该写三个值，每个值分别赋值给对应通道内的所有矩阵元素，如果出现参数比通道数少的情况，那么未赋值的通道内所有矩阵元素全部为0。另科普Mat类型的type参数：1.bit_depth：比特数，代表8bite,16bites,32bites,64bites例如创建一个存储灰度图片的Mat对象,这个图像的大小为宽100,高100,那么,现在这张灰度图片中有10000个像素点，它每一个像素点在内存空间所占的空间大小是8bite,8位，所以它对应的就是CV_82.S|U|FS代表signed int有符号整形U代表unsigned int无符号整形F代表float单精度浮点型3.C（channel）代表一张图片的通道数灰度图片grayImg是单通道图像RGB彩色图像是–3通道图像带Alph通道的RGB图像是4通道图像 二、Rect()函数函数定义：1234567typedef struct Rect &#123; int x; /* 方形的左上角的x-坐标 */ int y; /* 方形的左上角的y-坐标*/ int width; /* 宽 */ int height; /* 高 */ &#125;函数用法：123456789101112131415161718192021222324252627282930313233343536373839//如果创建一个Rect对象rect(100, 50, 50, 100)，那么rect会有以下几个功能： rect.area(); //返回rect的面积 5000 rect.size(); //返回rect的尺寸 [50 × 100] rect.tl(); //返回rect的左上顶点的坐标 [100, 50] rect.br(); //返回rect的右下顶点的坐标 [150, 150] rect.width(); //返回rect的宽度 50 rect.height(); //返回rect的高度 100 rect.contains(Point(x, y)); //返回布尔变量，判断rect是否包含Point(x, y)点 //还可以求两个矩形的交集和并集 rect = rect1 &amp; rect2; rect = rect1 | rect2; //还可以对矩形进行平移和缩放 rect = rect + Point(-100, 100); //平移，也就是左上顶点的x坐标-100，y坐标+100 rect = rect + Size(-100, 100); //缩放，左上顶点不变，宽度-100，高度+100 //还可以对矩形进行对比，返回布尔变量 rect1 == rect2; rect1 != rect2; //判断rect1是否在rect2里面bool isInside(Rect rect1, Rect rect2) &#123; return (rect1 == (rect1&amp;rect2)); &#125; //矩形中心点Point getCenterPoint(Rect rect) &#123; Point cpt; cpt.x = rect.x + cvRound(rect.width/2.0); cpt.y = rect.y + cvRound(rect.height/2.0); return cpt; &#125; //围绕矩形中心缩放 Rect rectCenterScale(Rect rect, Size size) &#123; rect = rect + size; Point pt; pt.x = cvRound(size.width/2.0); pt.y = cvRound(size.height/2.0); return (rect-pt); &#125; Rect()函数用法摘自：【OpenCV】cv::Rect矩形类用法blog.csdn.net/qq_30214939/article/details/65648273 三、Rectangle()函数函数原型：12345678void rectangle(InputOutputArray img, Point pt1, Point pt2,const Scalar&amp; color, int thickness = 1,int lineType = LINE_8, int shift = 0);第一个参数是要处理的图片第二和第三个参数分别是矩形的左上角和右下角的坐标第四个参数是矩形的颜色第五个参数是线的粗细第六个参数是线形 四、RNG随机数类型RNG可以产生3种随机数：RNG(int seed)使用种子seed产生一个64位随机整数，默认-1（计算机的伪随机数是由随机种子根据一定的计算方法计算出来的数值，所以只要计算方法一定，随机种子一定，那么产生的随机数就是固定的）RNG::uniform()产生一个均匀分布的随机数（RNG::uniform(a, b )返回一个[a,b)范围的均匀分布的随机数，a,b的数据类型要一致，而且必须是int、float、double中的一种，默认是int）RNG::gaussian( )产生一个高斯分布的随机数（返回一个均值为0，标准差为σ的随机数。如果要产生均值为λ，标准差为σ的随机数，可以λ+ RNG::gaussian( σ)） 五、Mat&amp; image = (Mat) param;的解释param是用户定义的传递到setMouseCallback()函数调用的参数,在本行代码中先将param强制转换为Mat类型指针，然后取param的值赋值给左边作为Mat类型引用的image变量。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[滑动条创建和使用]]></title>
    <url>%2F2019%2F03%2F16%2FOpenCV-Learning-Day-6%2F</url>
    <content type="text"><![CDATA[前言没有前言。 测试代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;const int g_nMaxAlphaValue = 100; //Alpha值最大值int g_nAlphaValueSlider;//滑动条对应变量double g_dAlphaValue;double g_dBetaValue;Mat g_srcImage1;Mat g_srcImage2;Mat g_dstImage;void on_Trackbar(int, void*)&#123;//Alpha和Beta值的比例g_dAlphaValue = (double) g_nAlphaValueSlider / g_nMaxAlphaValue;g_dBetaValue = (1.0 - g_dAlphaValue);//根据Alpha和Beta线性混合addWeighted(g_srcImage1,g_dAlphaValue,g_srcImage2,g_dBetaValue,0,g_dstImage);imshow("Trackbar Sample",g_dstImage);&#125;int main(int argc,char** argv)&#123;//图像尺寸必须相同！！！g_srcImage1 = imread("Paint1.jpg");g_srcImage2 = imread("Paint2.jpg");if(!g_srcImage1.data) &#123;printf("Reading image 1 failed\n");return -1;&#125;if(!g_srcImage2.data) &#123;printf("Reading image 2 failed\n");return -1;&#125;g_nAlphaValueSlider = 70;namedWindow("Trackbar Sample",1);char TrakbarName[50];sprintf(TrakbarName,"Alpha");createTrackbar(TrakbarName,"Trackbar Sample",&amp;g_nAlphaValueSlider,g_nMaxAlphaValue,on_Trackbar);//运行时初始化界面on_Trackbar(g_nAlphaValueSlider,0);waitKey(0);return 0;&#125;效果图；Alpha为0时：Alpha为100时： 1.createTrackbar() 函数函数原型：123456int createTrackbar(const string&amp; trackbarname, const string&amp; winname, int* value, int count, TrackbarCallback onChange=0, void* userdata=0)第一个参数：轨迹条的名字第二个参数：窗口的名字第三个参数：int类型的指针，指向滑块位置，创建时滑块的位置就是该变量当前的值第四个参数：滑块能达到的最大位置的值第五个参数：TrackbarCallback回调函数，默认为零，滑动条的每一次变化都会调用这个函数，函数原型必须是void XXX(int, void)，第一个参数是轨迹条的位置，第二个参数是用户传给回调函数的参数，如果第三个参数是全局变量的话，完全可以不用管第六个参数。需要注意的是，读取的图像尺寸必须相同，不然会报错。2.getTrackbarPos()：获取滑动条的位置的值函数原型：123int getTrackbarPos(const string&amp; trackbarname, const string&amp; winname)]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[图像的简单载入、显示和输出]]></title>
    <url>%2F2019%2F03%2F15%2FOpenCV-Learning-Day-5%2F</url>
    <content type="text"><![CDATA[前言本章主要介绍图像的简单载入、显示和输出，新内容有namedWindow()函数、图像的ROI（感兴趣区域）以及图像的输出imwrite()函数，顺带科普一下c++文件中常常定义在main函数中的两个形参argc和argv究竟是什么意思～ 一、图像的载入、显示和输出测试代码：123456789101112131415161718192021222324252627282930#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;int main()&#123;Mat image = imread("Girl.jpg");Mat logo = imread("logo.jpg");namedWindow("Girl");imshow("Girl",image);namedWindow("logo");imshow("logo",logo);Mat imageROI;imageROI = image(Rect(1600,2100,logo.cols,logo.rows));//imageROI = image(Range(2100,2100+logo.rows),Range(1600,1600+logo.cols));addWeighted(imageROI,0.5,logo,1,0,imageROI);namedWindow("Girl+logo");imshow("Girl+logo",image);imwrite("Output.jpg",image);waitKey(0);return 0;&#125; 原图： 效果图； 1.为什么会有namedWindow()函数？在简单的图像处理中，我们通常只要读取图像，经过处理后使用imshow()函数输出图像即可，但是在进阶操作中，我们可能会在执行代码生成的窗口上放置滚动条或者按钮，这时候我们就需要在代码执行前就定义这个窗口，这就是namedWindow()函数的作用。函数原型：12void nameWindow(const string&amp; winname,int flags = WINDOW_AUTOSIZE)其中第二个参数可以设置窗口能否放缩以及是否支持OpenGL。2.ROI（region of interest）感兴趣区域圈定一块图像中需要处理的区域，既节省性能，又便于操作。需要定义一个Mat类型以存放图像的ROI，设置ROI其实就是在原来图片上指定一个区域，而这个区域只是新创建了一个图片文件的头信息而已并没有产生新的图片，文件头里的图片区域的起始位置指向了ROI区域的左上角位置，所以在ROI上做的任何操作都会影响原图片。设置ROI区域有两种方法，测试代码里的注释是另一种，两者效果相同。3.imwrite()函数：输出图像到文件。函数原型：123bool imwrite(const string&amp; filename, InputArray img, const vector&lt;int&gt;&amp; params=vector&lt;int&gt;() )注意：第一个参数文件名要带上后缀 二、argc和argvargc、argv中的arg指的是参数（argument），因此argc的全称为argument counter和argument vector，其中argc为整数，用来统计运行程序时送给main函数的命令行参数的个数；而*argv[]:为字符串数组用来存放指向的字符串参数的指针数组，每一个元素指向一个参数。各成员含义如下：argv[0]指向程序运行的全路径名argv[1]指向在命令行中执行程序名后的第一个字符串argv[2]指向在命令行中执行程序名后的第二个字符串argv[3]指向在命令行中执行程序名后的第三个字符串argv[argv]为NULL]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基本视频读取和操作]]></title>
    <url>%2F2019%2F03%2F14%2FOpenCV-Learning-Day-4%2F</url>
    <content type="text"><![CDATA[前言本文主要介绍VideoCapture()函数的使用，包括读取视频和调用摄像头等操作，并结合之前学的一些基本图像处理操作实现视频中物体边缘显示并给出测试代码。对于Mac OS X 10.14无法调用摄像头的问题，文中将给出解决方法。 一、读取视频测试代码：1234567891011121314151617#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;int main()&#123;VideoCapture capture("Adventure.mp4");//循环显示每一帧while(1)&#123;Mat frame;capture &gt;&gt; frame;imshow("Video",frame);waitKey(30);&#125;return 0;&#125; 对于声明为VideoCapture类型的参数，只要将其初始化为视频文件的相对路径或者绝对路径，就可以读取视频文件。定义一个Mat类型的变量，通过while循环将视频的每一帧读取到Mat变量中，这样就可以实现图像操作。 二、调用摄像头测试代码：1234567891011121314151617181920212223242526#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;int main()&#123;VideoCapture capture(0);//循环显示每一帧while(1)&#123;Mat srcImage;capture &gt;&gt; srcImage;Mat grayImage, dstImage;Mat grad_x,abs_grad_x,grad_y,abs_grad_y;cvtColor(srcImage,grayImage,COLOR_BGR2GRAY);blur(grayImage,dstImage,Size(3,3));Sobel(grayImage,grad_x,CV_16S,1,0,3);convertScaleAbs(grad_x,abs_grad_x);Sobel(grayImage,grad_y,CV_16S,0,1,3);convertScaleAbs(grad_y,abs_grad_y);addWeighted(abs_grad_x,0.5,abs_grad_y,0.5,0,dstImage);imshow("dstImage",dstImage);waitKey(30);&#125;return 0;&#125; 对于声明为VideoCapture类型的参数，只要将其初始化为0，就表示调用摄像头。定义一个Mat类型的变量，通过while循环将视频的每一帧读取到Mat变量中，这样就可以实现图像操作。测试代码实现的功能是调用摄像头后通过sobel算子勾勒出物体轮廓。具体sobel函数实现可参考我的上一篇博客：三种基本边缘检测算子www.whoiscaesarbao.com/2019/03/13/OpenCV-Learning-Day-3需要注意的是，Mac OS X 10.14版本存在摄像头无法调用的问题，原因是摄像头权限没给，因此我们要在应用程序文件夹里找到macOS自带的Photo Booth.app，右键选择显示包内容，文件夹里有一个Info.plist文件，默认使用Xcode打开并删去不必要的项，并加入最后一项Privacy - Camera Usage Description 设置为YES，配置后如下图：其中OpenCV是我的项目名，将编辑好的Info.plist文件拷贝到：Xcode为与项目文件夹并列VS Code为与生成的可执行文件并列这样就有权限调用摄像头了。 今天有点水，做数学作业去了～]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[三种基本边缘检测算子]]></title>
    <url>%2F2019%2F03%2F13%2FOpenCV-Learning-Day-3%2F</url>
    <content type="text"><![CDATA[前言：本片主要介绍三个基本边缘检测的算子（canny算子、sobel算子、laplace算子），关于三个算子的原理和介绍ck2016的简书博客：数字图像 - 边缘检测原理 - Sobel, Laplace, Canny算子www.jianshu.com/p/2334bee37de5 介绍的非常通俗易懂和详细，下面主要放函数原型，测试代码和图片展示～ 原图： 一、canny算子：函数原型：1234567891011void cv::Canny ( InputArray image,//输入图像，必须为单通道灰度图OutputArray edges,//输出图像，为单通道黑白图double threshold1,double threshold2,//第三个参数和第四个参数表示阈值，这二个阈值中当中的小阈值用来控制边缘连接，大的阈值用来控制强边缘的初始分割，即如果一个像素的梯度大于上限值，则被认为是边缘像素，如果小于下限阈值，则被抛弃。如果该点的梯度在两者之间则当这个点与高于上限值的像素点连接时我们才保留，否则删除。int apertureSize = 3,//第五个参数表示Sobel 算子大小，默认为3即表示一个3*3的矩阵。Sobel 算子与高斯拉普拉斯算子都是常用的边缘算子bool L2gradient = false ) 测试代码：1234567891011121314151617181920#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123;Mat srcImage = imread("Girl.jpg");imshow("srcImage",srcImage);Mat grayImage, dstImage;cvtColor(srcImage,grayImage,COLOR_BGR2GRAY);blur(grayImage,dstImage,Size(3,3));Canny(dstImage,dstImage,3,9,3);imshow("dstImage",dstImage);waitKey(0);&#125; 效果图： 需要注意的是，canny算子在算出梯度值后会有勾勒边缘的一步：把区域内不是极值的点全部置0，因此效果图中的边缘会变成细线，但同时也会导致一些弱的边缘会被抹去，因此canny算子提供了两个阈值的设置，数值超过大阈值的像素点会被认为是边缘，低于小阈值的像素点被认为不是边缘，数值处于两个阈值之间的，将由周围已经被认为是边缘的像素点开始走格子，可达的是边缘，不可达的被认为不是。阈值的大小比最好为2：1或3：1。 二、sobel算子函数原型：12345678910void Sobel( InputArray src, //输入图像OutputArray dst, //输出图像int ddepth,//目标图像的颜色深度int dx, //取1表示对x求一阶导数，用来检测竖直边缘int dy, //取1表示对y求一阶导数，用来检测水平边缘int ksize=3, //sobel核的大小，必须是奇数，默认为3double scale=1, double delta=0,int borderType=BORDER_DEFAULT );测试代码：123456789101112131415161718192021222324252627#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123;Mat srcImage = imread("Girl.jpg");imshow("scrImage",srcImage);Mat grayImage, dstImage;Mat grad_x,abs_grad_x,grad_y,abs_grad_y;//取灰度图和模糊降噪cvtColor(srcImage,grayImage,COLOR_BGR2GRAY);blur(grayImage,dstImage,Size(3,3));Sobel(grayImage,grad_x,CV_16S,1,0,3); //sobel算子使用16位有符号的数据类型,防止截断convertScaleAbs(grad_x,abs_grad_x); //用convertScaleAbs()函数将其转回原来的uint8形式Sobel(grayImage,grad_y,CV_16S,0,1,3);convertScaleAbs(grad_y,abs_grad_y);addWeighted(abs_grad_x,0.5,abs_grad_y,0.5,0,dstImage);imshow("dstImage",dstImage);waitKey(0);&#125; 效果图： 1.需要注意的是，sobel算子的颜色深度最好使用CV_16S，因为OpenCV文档中对Sobel算子的介绍中有这么一句：“in the case of 8-bit input images it will result in truncated derivatives”。即Sobel函数求完导数后会有负值，还有会大于255的值。而原图像是uint8，即8位无符号数，所以Sobel建立的图像位数不够，会有截断，因此要使用16位有符号的数据类型，即cv2.CV_16S。因此在计算完之后要用convertScaleAbs()函数将其转回原来的uint8形式。2.addWeighted()函数是将两张相同大小，相同类型的图片融合的函数。 函数原型：1234567void cvAddWeighted( const CvArr* src1, //第一个原数组double alpha,//第一个数组元素权重const CvArr* src2, //第二个原数值double beta,//第二个数组元素权重double gamma, //两个数组作和后添加的数值。不要太大，不然图片一片白。总和等于255以上就是纯白色了CvArr* dst );//输出图像 三、laplace算子函数原型：12345678void Laplacian( src_gray, //输入图像dst, //输出图像ddepth, //因为输入图像一般为CV_8U，为了避免数据溢出，输出图像深度应该设置为CV_16Skernel_size, //核大小，默认为3scale, delta, BORDER_DEFAULT); 测试代码：123456789101112131415161718192021222324#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123;Mat srcImage = imread("Girl.jpg");imshow("srcImage",srcImage);Mat grayImage, dstImage;//取灰度图和模糊降噪cvtColor(srcImage,grayImage,COLOR_BGR2GRAY);blur(grayImage,dstImage,Size(3,3));Laplacian(grayImage,dstImage,CV_16S,3);convertScaleAbs(dstImage,dstImage);imshow("dstImage",dstImage);waitKey(0);&#125; 效果图： 需要注意的是，由于输入图像是CV_8U格式，但是在使用laplace是为了防止截断需要转换成CV_16S格式，在计算完之后需要convertScaleAbs()函数转换回CV_8U格式，否则是一张灰色图像，别问我怎么知道的。 总结：sobel算子产生的边缘有强弱，抗噪性好。laplace算子对边缘敏感，可能有些是噪声的边缘，也被算进来了。canny算走产生的边缘很细，可能就一个像素那么细，没有强弱之分。 参考资料：【OpenCV入门教程之十二】OpenCV边缘检测：Canny算子,Sobel算子,Laplace算子,Scharr滤波器合辑blog.csdn.net/poem_qianmo/article/details/25560901 数字图像 - 边缘检测原理 - Sobel, Laplace, Canny算子www.jianshu.com/p/2334bee37de5 OpenCV-Python教程（6、Sobel算子）blog.csdn.net/sunny2038/article/details/9170013 Opencv--Sobel算子blog.csdn.net/qq_41248872/article/details/82886228 opencv中addWeighted()函数用法总结（05）blog.csdn.net/fanjiule/article/details/81607873]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[五种基本滤波器]]></title>
    <url>%2F2019%2F03%2F12%2FOpenCV-Learning-Day-2%2F</url>
    <content type="text"><![CDATA[前言：本片主要介绍五个常见的滤波器（线性滤波、方框滤波、高斯滤波、中值滤波、双边滤波），文章基本是我翻阅博客整理而成的笔记～目前水平太菜了自己写不了基础知识，只能整理大神们的文章了！ 测试代码：1234567891011121314151617181920212223242526272829#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123;Mat srcImage = imread("Bilbo.jpg");imshow("Input Image",srcImage);Mat dstImage;blur(srcImage,dstImage,Size(3,3));imshow("blur",dstImage);boxFilter(srcImage,dstImage,-1,Size(3,3));imshow("boxFilter",dstImage);GaussianBlur(srcImage,dstImage,Size(3,3),0,0);imshow("GaussianBlur",dstImage);medianBlur(srcImage,dstImage,3);imshow("medianBlur",dstImage);bilateralFilter(srcImage,dstImage,100,0,0);imshow("bilateralFilter",dstImage);waitKey(0);&#125; 效果图： 一、基本滤波器1.线性滤波均值滤波blur：将一个区域内的像素值求和取平均值，然后用这个平均值替换区域中心的像素值。计算速度很快，但是在去躁的同时会模糊很多细节部分，不容易保存细节。 函数原型：12345void blur( InputArray src, //输入图像OutputArray dst, //输出图像Size ksize, //内核大小Point anchor = Point(-1,-1), //锚点位置，默认为中心点int borderType = BORDER_DEFAULT ); //用于推断图像外部像素的某种边界模式，默认值BORDER_DEFAULT 效果图： 方框滤波boxFilter：算法和均值滤波相似，优缺点也相同，均值滤波算是方框滤波的特殊版本。 效果图： 高斯滤波GaussianBlur：高斯滤波是专门用于消除满足高斯分布(正态分布)的误差而存在的滤波，此时邻域算子是专门的高斯核，图像中的像素与高斯核做卷积，生成的结果加权平均存放到目标像素中，权重按照二维正态分布，对于抑制符合正态分布的噪声非常有效，并可以增强图像值不同比例下的图像效果。 函数原型：12345678void GaussianBlur(InputArray src, OutputArray dst, Size ksize, //表示高斯核函数在Y方向的的标准偏差，若sigmaY为零，就将它设为sigmaX，如果sigmaX和sigmaY都是0，那么就由ksize.width和ksize.height计算出来double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT); 效果图： 2.非线性滤波中值滤波medianBlur：基本思想就是用像素点的领域灰度的中值来代替该像素点的灰度值，该方法在去除脉冲噪声、椒盐噪声的同时又能保留图像的细节（不会出现边缘模糊的情况）。中值滤波跟均值滤波的思想看起来很相似，只是一个取平均值，一个取中位数，但是均值滤波的计算速度是中值滤波的五倍，具体原因可以看总结。 函数原型：1234void medianBlur(InputArray src,OutputArray dst, int ksize) //孔径的线性尺寸（aperture linear size），必须为奇数 效果图： 双边滤波bilateralFilter：双边滤波的最大特点就是做边缘保存，保边去噪。大概的原理是给定一个范围，如果某些区域点与点之间如果数值差距超出范围，那么这个区域就不滤波了，能够很好的保留图像轮廓。函数原型：1234567void bilateralFilter(InputArray src, OutputArray dst, int d, //在过滤过程中每个像素邻域的直径范围double sigmaColor, //颜色空间过滤器的sigma值double sigmaSpace, //坐标空间中滤波器的sigma值int borderType=BORDER_DEFAULT ) //用于推断图像外部像素的某种边界模式，默认值BORDER_DEFAULT 注：对sigmaColor、和sigmaSpace两个参数存在比较大的疑惑，因为我不管怎么调后面的参数，效果图好像和原图没有差别…效果图： 3.线性滤波和非线性滤波的区别线性滤波器的原始数据与滤波结果是一种算术运算，即用加减乘除等运算实现，如均值滤波器（模板内像素灰度值的平均值）、高斯滤波器（高斯加权平均值）等。由于线性滤波器是算术运算，有固定的模板，因此滤波器的转移函数是可以确定并且是唯一的（转移函数即模板的傅里叶变换）。 非线性滤波器的原始数据与滤波结果是一种逻辑关系，即用逻辑运算实现，如最大值滤波器、最小值滤波器、中值滤波器等，是通过比较一定邻域内的灰度值大小来实现的，没有固定的模板，因而也就没有特定的转移函数（因为没有模板作傅里叶变换），另外，膨胀和腐蚀也是通过最大值、最小值滤波器实现的。（这就是为什么均值滤波要比中值滤波快五倍的原因：需要通过排序确定中值后才能生成模版） 参考资料：什么是线性滤波、非线性滤波https://www.cnblogs.com/snowxshy/p/3855011.htmlopencv中的各种滤波https://www.jianshu.com/p/8f4024742821OpenCv基本滤波算法小结 blockquotehttps://blog.csdn.net/fzhykx/article/details/79546549opencv中的各种滤波函数https://blog.csdn.net/zoucharming/article/details/70197863OpenCV探索之路（三）：滤波操作https://www.cnblogs.com/skyfsm/p/6873188.html]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[了解卷积：深度学习中的卷积之我见]]></title>
    <url>%2F2019%2F03%2F11%2FSomething-Fundamental-about-Convolution%2F</url>
    <content type="text"><![CDATA[一、前言最近在学习OpenCV的过程中接触到了几个常见的算子（canny算子、sobel算子、laplace算子），并对其中的卷积产生了非常大的困惑，此为其一；同时，卷积也可以说是深度学习当中最重要的概念，如CNN神经网络（Conventional Neural Net）更是深度学习中最为常见，也是最为优秀的神经网络之一，此为其二。可以说，卷积是一道不得不迈的门槛，迈过去能办事，迈不过去就只能吹牛了。在翻阅许多博客之后，我发现每个人对卷积的理解都不尽相同，每一篇都能让我有或多或少的收获，但没有一篇能让我完全理解，因此我想站在自己的角度上，稍微谈谈我对卷积以及卷积在深度学习中的使用的一些浅见。 二、一维卷积及其意义在数学中，卷积地表示是一维的。我们称 (f*g)(n) 为 f,g 的卷积，其连续的定义为：(f∗g)(n)=∫∞−∞f(τ)g(n−τ)dτ其离散的定义为：(f∗g)(n)=∑−∞∞f(τ)g(n−τ) 好家伙，看是看得懂，可卷积出来的结果究竟是个什么东西？它的意义是什么？我看到的最能让我明白的是这两个答案：如何通俗易懂地解释卷积？ - 张俊博的回答 - 知乎https://www.zhihu.com/question/22298352/answer/34267457卷积为什么叫「卷」积？ - 荆哲的回答 - 知乎https://www.zhihu.com/question/54677157/answer/141245297 看完你理解一下就会知道，卷积的一个重要意义就是：一个函数在另一个函数上的加权叠加。在深度学习中，我们把一个函数看作是输入的图像代表的矩阵（Input Image），把另一个图像看作是卷积核（Convolution Kernel），两者卷积，就是输入图像和卷积核的加权叠加，权重越大，卷积值越大，就代表越重要。这是一个陌生的概念，我先按下不提。我之所以会提及一维卷积，是因为一维的卷积更能帮助我们理解卷积的意义：加权叠加。这就是为什么要用卷积的原因所在。 三、人类视觉原理在谈及卷积在深度学习的应用之前，我想先分享一下深度学习神经网络是怎么出现的，这是摘自Alex Cai的博客http://www.cnblogs.com/alexcai/p/5506806.html它更能帮助我们了解神经网络的作用，以及卷积在其中扮演的角色。 深度学习的许多研究成果，离不开对大脑认知原理的研究，尤其是视觉原理的研究。 1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”，可视皮层是分级的。 人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。下面是人脑进行人脸识别的一个示例： 对于不同的物体，人类视觉也是通过这样逐层分级，来进行认知的： 我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。 那么我们可以很自然的想到：可以不可以模仿人类大脑的这个特点，构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类呢？答案是肯定的，这也是许多深度学习算法（包括CNN）的灵感来源。 四、深度学习中的卷积由上一节可见，机器学习脱胎于人类视觉原理，而卷积的作用，就是把关键细节抽象化，因为同一类事物总是有各种相同或者相似的细节，记得卷积的作用吗？加权叠加可以提取出相同的、我们所需要的细节并强调它，一层层的提取和强调，就可以生成机器识物的“逻辑”。 在深度学习中，我们所使用的卷积是二维的，因为机器所“看”的图片实质上是一个矩阵，那么相应的，卷积核也是一个矩阵，这个卷积核在 2 维输入数据上“滑动”，对当前输入的部分元素进行矩阵乘法，然后将结果汇为单个输出像素。 这里曾是让我最百思不得其解的地方，可能是智商限制了我的发挥，很长时间有一个问题困扰着我：一维卷积和二维卷积有什么关系？为什么二维卷积会这么计算而不是别的什么方法？其实稍微转换一下角度就很好理解：把输入图像的矩阵和卷积核排成一行而不是将它以二维的形式放置，就会发现它们其实就是f(x)和g(x)以离散的形式所构成的函数！实质上图像正是以这样一种方式，通过训练得出的卷积核，来抽象出图像的特征：强调个性，抑制共性，抽丝剥茧出一套“逻辑”来。 五、后记以上是我在一天的学习后所做的总结，本文没有涉及任何高级的操作，只是我对卷积的一些浅显认识过程。我认为，在深度学习中，打好基础，掌握必要概念非常重要，它会对后面的学习起到意想不到的效果！希望能给予想要学习深度学习的读者一个比较清楚的认识！ 参考链接：如何通俗易懂地解释卷积？ - 张俊博的回答 - 知乎https://www.zhihu.com/question/22298352/answer/34267457卷积为什么叫「卷」积？ - 荆哲的回答 - 知乎https://www.zhihu.com/question/54677157/answer/141245297Alex Cai的博客http://www.cnblogs.com/alexcai/p/5506806.html以及所有我看过的博客们！转载请注明出处]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[图像初级操作]]></title>
    <url>%2F2019%2F03%2F10%2FOpenCV-Learning-Day-1%2F</url>
    <content type="text"><![CDATA[本文内容包括：一、erode 图像腐蚀（dilate 图像膨胀）二、blur 均值滤波三、canny边缘检测 文末提供测试图片 一、erode 图像腐蚀（dilate 图像膨胀）1234567891011121314151617#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123;Mat srcImage = imread("Phone.png");imshow("former",srcImage);Mat element = getStructuringElement(MORPH_RECT,Size(15,15));Mat dstImage;erode(srcImage,dstImage,element);imshow("later",dstImage);waitKey(0);return 0;&#125; 1.imread函数只能读取绝对路径，不能读取相对路径，存在疑问。已解决：cmake编译时出现问题，需在CMakeLists.txt中加入一行1aux_source_directory(. DIR_SRCS)对整个文件夹进行扫描即可读取到图片的相对路径。（Xcode肯定没这问题但是我不想用哈哈～谁叫它长得没有VS Code好看） 2.getStructuringElement函数原型：1Mat getStructuringElement(int shape, Size esize, Point anchor = Point(-1, -1));函数的第一个参数表示内核的形状，有三种形状可以选择。矩形：MORPH_RECT;交叉形：MORPH_CORSS;椭圆形：MORPH_ELLIPSE;第二和第三个参数分别是内核的尺寸以及锚点的位置。一般在调用erode以及dilate函数之前，先定义一个Mat类型的变量来获得getStructuringElement函数的返回值。对于锚点的位置，有默认值Point（-1,-1），表示锚点位于中心点。element形状唯一依赖锚点位置，其他情况下，锚点只是影响了形态学运算结果的偏移。 3.erode 图像腐蚀（dilate 图像膨胀）erode 函数原型：12void erode( const Mat&amp; src, Mat&amp; dst, const Mat&amp; element,Point anchor=Point(-1,-1), int iterations=1,int borderType=BORDER_CONSTANT,const Scalar&amp; borderValue=morphologyDefaultBorderValue() );dilate 函数原型：12void dilate( const Mat&amp; src, Mat&amp; dst, const Mat&amp; element,Point anchor=Point(-1,-1), int iterations=1,int borderType=BORDER_CONSTANT,const Scalar&amp; borderValue=morphologyDefaultBorderValue() );参数：src:原图像。dst：目标图像。element:腐蚀操作的内核。 如果不指定，默认为一个简单的 3x3 矩阵。否则，我们就要明确指定它的形状，可以使用函数getStructuringElement().anchor:默认为Point(-1,-1),内核中心点。省略时为默认值。iterations:腐蚀次数。省略时为默认值1。borderType:推断边缘类型，具体参见borderInterpolate函数。默认为BORDER_DEFAULT，省略时为默认值。borderValue:边缘值，具体可参见createMorphoogyFilter函数。可省略。 二、blur 均值滤波1234567891011121314#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123;Mat srcImage = imread("Phone.png");imshow("src",srcImage);Mat dstImage;blur(srcImage,dstImage,Size(7,7));imshow("dst",dstImage);waitKey(0);&#125; 均值滤波是一种典型的线性滤波算法，主要是利用像素点邻域的像素值来计算像素点的值。其具体方法是首先给出一个滤波模板kernel，该模板将覆盖像素点周围的其他邻域像素点，去掉像素本身，将其邻域像素点相加然后取平均值即为该像素点的新的像素值，这就是均值滤波的本质。函数原型：1void blur(InputArray src, OutputArray dst, Size ksize, Point anchor=Point(-1,-1), int borderType=BORDER_DEFAULT);参数解释：InputArray src: 输入图像OutputArray dst: 输出图像Size ksize: 滤波模板kernel的尺寸，如Size(3,3)Point anchor=Point(-1, -1): 字面意思是锚点，也就是处理的像素位于kernel的什么位置，默认值为(-1, -1)即位于kernel中心点，如果没有特殊需要则不需要更改int borderType=BORDER_DEFAULT: 用于推断图像外部像素的某种边界模式，有默认值BORDER_DEFAULT 三、canny边缘检测12345678910111213141516171819202122232425262728#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123;Mat srcImage = imread("Phone.png");//原图像imshow("src",srcImage);Mat dstImage,edge,grayImage;//创建与srcImage同大小和同类型的矩阵dstImage.create(srcImage.size(),srcImage.type());//转换成灰度图像cvtColor(srcImage,grayImage,COLOR_BGR2GRAY);//使用滤波函数降噪，这里用的是10*10内核blur(grayImage,edge,Size(10,10));//运行canny算子Canny(edge,edge,3,9,3);//目标图像imshow("dst",edge);waitKey(0);&#125; 1.cvtColor颜色空间转换函数函数原型：123456void cvtColor( InputArray src, // 输入图像 OutputArray dst, // 输出图像 int code, // 颜色映射码 int dstCn = 0 // 输出的通道数 (0='automatic') );cvtColor()支持多种颜色空间之间的转换，目前常见的颜色空间均支持，并且在转换的过程中能够保证数据的类型不变，即转换后的图像的数据类型和位深与源图像一致。 2.canny算法Canny边缘检测于1986年由JOHN CANNY首次在论文《A Computational Approach to Edge Detection》中提出，就此拉开了Canny边缘检测算法的序幕。Canny边缘检测是从不同视觉对象中提取有用的结构信息并大大减少要处理的数据量的一种技术，目前已广泛应用于各种计算机视觉系统。函数原型：1234567891011void cv::Canny ( InputArray image,//输入图像，必须为单通道灰度图OutputArray edges,//输出图像，为单通道黑白图double threshold1,double threshold2,//第三个参数和第四个参数表示阈值，这二个阈值中当中的小阈值用来控制边缘连接，大的阈值用来控制强边缘的初始分割，即如果一个像素的梯度大于上限值，则被认为是边缘像素，如果小于下限阈值，则被抛弃。如果该点的梯度在两者之间则当这个点与高于上限值的像素点连接时我们才保留，否则删除。int apertureSize = 3,//第五个参数表示Sobel 算子大小，默认为3即表示一个3*3的矩阵。Sobel 算子与高斯拉普拉斯算子都是常用的边缘算子bool L2gradient = false ) 阈值的大小比最好为2：1或3：1。 测试图像：]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mac 使用VS Code 通过cmake 配置 OpenCV和Pytorch C++ API]]></title>
    <url>%2F2019%2F03%2F05%2FDeep-Learning-Starting%2F</url>
    <content type="text"><![CDATA[前情由于pytorch的1.0正式版发布不久，同时较为稳定的c++的API也是在正式版中提供支持，网上的教程不多，因此可供参考的资料只有官方文档和零零散散的博客。不过由于Mac和Linux本身相差不大，一些Linux的配置教程同样值得参考。以下给出链接：官方文档https://pytorch.org/cppdocs/installing.htmlOldpan的个人博客（特别感谢Oldpan老哥的帖子给我的巨大帮助）：利用Pytorch的C++前端(libtorch)读取预训练权重并进行预测：https://oldpan.me/archives/pytorch-c-libtorch-inferencePytorch源码编译简明指南：https://m.oldpan.me/archives/pytorch-build-simple-instruction 开始所需安装的工具有：1.VS Code：这个官网下载即可2.OpenCV 4.0.1:终端输入1brew install opencv即可安装3.Pytorch1.0:可参考上面给出的Pytorch源码编译简明指南，首先在终端输入1git clone --recursive https://github.com/pytorch/pytorch获取最新源码，然后通过编译得到Mac可以读取的.dylib文件（注意：在官方文档中下载的libtorch-shared-with-deps-latest.zip文件解压后所得到的文件夹里的动态库文件是以.so结尾，是Linux下的动态库文件，Mac识别不了）编译时应该先进入到刚刚下载好的Pytorch文件夹（默认的路径应该是/Users/用户名/pytorch）下，然后终端执行123mkdir buildcd buildpython ../tools/build_libtorch.py进行libtorch（即c++ API）的编译，时间较长。4.编译好之后打开VS Code新建一个工程，在这里我引用Oldpan老哥的例子 工程名叫simnet，然后在simnet文件夹下新建一个CMakeLists.txt和一个test.cpp（build先不建），CMakeLists.txt中的代码为：123456789101112131415161718192021cmake_minimum_required(VERSION 3.12 FATAL_ERROR)project(simnet)find_package(Torch REQUIRED) # 查找libtorchfind_package(OpenCV REQUIRED) # 查找OpenCVif(NOT Torch_FOUND)message(FATAL_ERROR "Pytorch Not Found!")endif(NOT Torch_FOUND)message(STATUS "Pytorch status:")message(STATUS " libraries: $&#123;TORCH_LIBRARIES&#125;")message(STATUS "OpenCV library status:")message(STATUS " version: $&#123;OpenCV_VERSION&#125;")message(STATUS " libraries: $&#123;OpenCV_LIBS&#125;")message(STATUS " include path: $&#123;OpenCV_INCLUDE_DIRS&#125;")add_executable(simnet test.cpp)target_link_libraries(simnet $&#123;TORCH_LIBRARIES&#125; $&#123;OpenCV_LIBS&#125;) set_property(TARGET simnet PROPERTY CXX_STANDARD 11) test.cpp中的代码为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;opencv2/opencv.hpp&gt;#include "torch/script.h"#include "torch/torch.h"#include &lt;iostream&gt;#include &lt;memory&gt;using namespace std;// resize并保持图像比例不变cv::Mat resize_with_ratio(cv::Mat&amp; img) &#123;cv::Mat temImage;int w = img.cols;int h = img.rows;float t = 1.;float len = t * std::max(w, h);int dst_w = 224, dst_h = 224;cv::Mat image = cv::Mat(cv::Size(dst_w, dst_h), CV_8UC3, cv::Scalar(128,128,128));cv::Mat imageROI;if(len==w)&#123;float ratio = (float)h/(float)w;cv::resize(img,temImage,cv::Size(224,224*ratio),0,0,cv::INTER_LINEAR);imageROI = image(cv::Rect(0, ((dst_h-224*ratio)/2), temImage.cols, temImage.rows));temImage.copyTo(imageROI);&#125;else&#123;float ratio = (float)w/(float)h;cv::resize(img,temImage,cv::Size(224*ratio,224),0,0,cv::INTER_LINEAR);imageROI = image(cv::Rect(((dst_w-224*ratio)/2), 0, temImage.cols, temImage.rows));temImage.copyTo(imageROI);&#125;return image;&#125;int main(int argc, const char* argv[])&#123;if (argc != 2) &#123;std::cerr &lt;&lt; "usage: example-app &lt;path-to-exported-script-module&gt;\n";return -1;&#125;cv::VideoCapture stream(0);cv::namedWindow("Gesture Detect", cv::WINDOW_AUTOSIZE);std::shared_ptr&lt;torch::jit::script::Module&gt; module = torch::jit::load(argv[1]);module-&gt;to(at::kCUDA);cv::Mat frame;cv::Mat image;cv::Mat input;while(1)&#123;stream&gt;&gt;frame;image = resize_with_ratio(frame);imshow("resized image",image); //显示摄像头的数据cv::cvtColor(image, input, cv::COLOR_BGR2RGB);// 下方的代码即将图像转化为Tensor，随后导入模型进行预测torch::Tensor tensor_image = torch::from_blob(input.data, &#123;1,input.rows, input.cols,3&#125;, torch::kByte);tensor_image = tensor_image.permute(&#123;0,3,1,2&#125;);tensor_image = tensor_image.toType(torch::kFloat);tensor_image = tensor_image.div(255);tensor_image = tensor_image.to(torch::kCUDA);torch::Tensor result = module-&gt;forward(&#123;tensor_image&#125;).toTensor();auto max_result = result.max(1, true);auto max_index = std::get&lt;1&gt;(max_result).item&lt;float&gt;();if(max_index == 0)cv::putText(frame, "paper", &#123;40, 50&#125;, cv::FONT_HERSHEY_PLAIN, 2.0, cv::Scalar(0, 255, 0), 2);else if(max_index == 1)cv::putText(frame, "scissors", &#123;40, 50&#125;, cv::FONT_HERSHEY_PLAIN, 2.0, cv::Scalar(0, 255, 0), 2);elsecv::putText(frame, "stone", &#123;40, 50&#125;, cv::FONT_HERSHEY_PLAIN, 2.0, cv::Scalar(0, 255, 0), 2);imshow("Gesture Detect",frame); //显示摄像头的数据cv::waitKey(30);&#125; 保存。 5.终端cd进入simnet工程文件夹，然后执行1234mkdir buildcd buildcmake -DCMAKE_PREFIX_PATH=/absolute/path/to/pytorch ..make其中/absolute/path/to/pytorch是pytorch文件夹的绝对路径，一般是/Users/用户名/pytorch这样就编译完成了可以执行1./simnet来运行你的工程了！ 后记由于这是我第一次写教程，很多地方可能有所疏漏，并且配置的过程中踩了无数的坑，可能很多地方起了效果但是我根本没有注意到，还有前期的一些准备工作我也没有提及（比如说anaconda的安装，pip、conda、brew的安装和更新），这些网上有很多大佬写的非常详尽的教程，大家可以多多参考，我这里只是提供了一个自己的思路，如果没有安装成功还望见谅！]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《La La Land》：有关闪耀而易碎的、遗憾而完美的那些事]]></title>
    <url>%2F2019%2F01%2F21%2FLand%2F</url>
    <content type="text"><![CDATA[City of stars, are you shining for me? LA，los angeles，city of stars，确实太过闪耀，有太多人，哪怕从未踏上洛杉矶的土地，也对那里根种了太多美好的幻想。我想起几年前我拿着我新买的PS4，打开《GTA V》的时候，我的天！主角Michael的家外边就是粼粼的海面和充满“热情”的沙滩，无云而湛蓝的天际延伸至远处渐渐变淡，与海面交融直至分不清楚。坐上红色跑车，手搭在车窗上，沿着海岸线一路向北，深红色的岩石蒸腾着热气，视线延伸向上看见绿色————是棕榈树————阳光刺眼，模模糊糊，看不真切；渐入钢铁丛林，阳光也是热辣的吓人，大片大片的反光，大片大片的明媚，西装革履的男人，潮流青年，穿休闲服遛狗的人，酒鬼，或昂首挺胸，或散散漫漫，或失魂落魄，穿梭在层层叠叠的阴影和光斑之间，完美融进这这城市里，一如他们迥然的处境和各不相同的命运。也许是Rockstar太过顶级的美工，又有可能是他们早年的作品圣安第列斯已经满足了我的杀戮欲望，现在我规规矩矩地在马路上开着我的车，老老实实的等着红灯熄灭，绿灯闪烁，循规蹈矩的和其他市民一般驾驶。再往上开，房子渐渐的又变矮了，地势升高，一块巨大的标牌从山顶慢慢向我挪来。我慢慢驶近，在不经意间，它突然变得明亮起来。 啊，HOLLY WOOD。 闪亮的灯牌提醒我天色已晚。我下了车，站在富人区的山顶上，眺望着整个圣洛都。细小闪耀的灯光如同沙粒，连着川流不息的车流，成了江成了河，向我诉说这座钢之巨人自他从这片泥土地里诞生起便永不入睡。是啊，它是如此勤奋地活着，连同它身体里的人们，仿佛都是永不入睡的，为了梦想，为了家庭，为了生活，抑或是吸了毒嗨了药，who cares？圣洛都的天空，是我见过的最美的天空————渐变色的，难以形容————或者可以想象一下，你现在正在一个晚宴上，和你心仪的人交谈，总有那么一两个时候你会觉得羞涩，心里有头小鹿撞来撞去，眼神不知道该往哪里看；于是你只好低头盯着你手里拿着的酒杯，因为这样显得你优雅而有礼貌，你开始端详葡萄酒的颜色：深红的葡萄酒被晚宴上昏暗的光线一打，自底向上由浅入深，不断的变换着色彩，可能是浅粉色，可能是淡蓝色，可能是深紫色；你挪动酒杯，颜色也跟着变化————是了，这就是LA的夜空，不仅仅是渐变的，柔和的，绚丽的，同时，它也是暧昧的，易碎的，不可告人的。 后来我打通了游戏，我最后一次看着Michael，Franklin，Trevor站在一起，看着车子渐渐沉进海底，三个人各自说着像总结一样的话，仿佛是隔着屏幕告诉我们他们的故事到这里就结束了，从今往后他们三个再在哪里相见，以什么样的方式相见，都不再关我的事，自北扬克顿拉开帷幕的故事到这里收场，生活还要继续；后来，因为学习，我（被强迫）收起了我的PS4，到那个时候我已经能轻车熟路的开遍圣洛都的大街小巷，我已然成为了一个老圣洛都人；可是那一晚，我打开我新买的PS4，打开我新买的《GTA V》，逛了逛海滩，坐上我的红色跑车，一路向北……那一幕幕带给我的震撼，我将永远不会忘记。因为是从那个时候起，我真正地将游戏打心底里视为一种艺术，而不是什么害人的消遣，到后来，我从网上知道原来GTA系列是描述美国梦的，我总是不由自主地和那个夜晚联系在一起——繁华的，易碎的，暧昧的，不可告人的……大概就是这样的吧。 我写了这么长的铺垫，其实是想告诉你，为什么我会对这部电影感触如此之深：不仅仅是因为我再次看到了那些似曾相识的场景，美丽的难以忘怀；而且我还看到了另一个故事，有关美国梦的故事，美好的令人心碎。一如这部电影的画面，我明知现实生活中不可能发生这样的故事，却根本不愿意去质疑。就让我带着迷蒙的的眼神，盯着那只葡萄酒杯；就让我微醺的脸色，更浓一分吧。 就当我做了一个梦，而我选择再不复醒。 还没写完先放放。]]></content>
      <categories>
        <category>Film Comment</category>
      </categories>
  </entry>
</search>
