<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[通过python实现官网监控、音乐播放、邮件提醒的简易程序]]></title>
    <url>%2F2020%2F03%2F26%2FPythonQiangbudaoFanxin%2F</url>
    <content type="text"><![CDATA[前言本弟弟是一个大二计算机专业学生，因为准备考研而起了买个iPad pro的念头，怎奈家境贫寒（那你能帮帮我吗？）只能起了买官翻的念头。但是官翻更新没有规律，经常手动刷新容易错过，因此受到吧里老哥的启发，用python写了一个监控官网的小程序，成功帮群里的几位老哥抢到翻新，证明了这个程序的效果。 1.尝试前第一，需要注意的是，由于本弟弟不会封装，因此尝试前请务必安装python3的环境，不可以因为没装环境运行不了留言或者私信骂我啥b！博主环境：macOS+pycharm+python3.7.4代码上import的包都可以用pip工具安装～第二，由于程序写的很简单，有的时候可能会根据不同地区不同家庭的网络状况而造成更新不及时的问题，博主也是失败了三次才成功抢到～不可以因为通知晚了没抢到留言或者私信骂我啥b！第三、博主分享这个程序，是希望能够激发各位懂行的老哥的兴趣，并探讨和交流，觉得有效果的可以捐赠我鼓励我！ 2.开始！一、发邮件函数如果你没有办法一直蹲在电脑前，就需要用到发邮件来通知你官网的更新。以QQ邮箱为例，这需要邮箱打开SMTP服务，具体可以进行百度，打开后获取授权码。1234567891011121314151617181920def send_mail(title, article, receiver): host = 'smtp.qq.com' # 这是QQ邮箱SMTP服务器的host，其他邮箱有不同可具体查询 user = 'XXXXXXXXX@qq.com'#这是邮箱号 password = 'XXXXXXXXXXXXXX'#这是授权码，注意不是邮箱的密码或者QQ的密码！ sender = user coding = 'utf8' message = MIMEText(article, 'plain', coding) message['From'] = Header(sender, coding) message['To'] = Header(receiver, coding) message['subject'] = Header(title, coding) try: mail_client = smtplib.SMTP_SSL(host, 465)#部分邮箱信道不同，又有可能没有开启SSL服务，具体查询 mail_client.connect(host) mail_client.login(user, password) mail_client.sendmail(sender, receiver, message.as_string()) mail_client.close() print('邮件已成功发送给:' + receiver) except: print('发送失败!')二、循环爬取网页信息123456789101112131415161718192021222324252627while True: print("Get Work!") localtime = time.asctime(time.localtime(time.time())) # 报时，免得程序卡住不知道～ print(localtime) url = 'https://www.apple.com.cn/shop/refurbished/ipad' # 这个是iPad翻新页面 headers = &#123;'User-Agent': 'Mozilla/5.0 3578.98 Safari/537.36'&#125; # 添加headers防止官网认为是爬虫而屏蔽访问 req = urllib.request.Request(url, headers=headers) try: rsp = urllib.request.urlopen(req) except: continue html = rsp.read().decode('utf-8', 'ignore') # print(html) html = BeautifulSoup(html, 'html.parser') for link in html.find_all('a'): info_link = link.get('href') info_text = link.get_text(strip=True) # print(info_text) # print(info_link + '\n') if info_text == "翻新 11 英寸 iPad Pro 无线局域网机型 64GB - 银色": print('Got 翻新 11 英寸 iPad Pro 无线局域网机型 64GB - 银色!') # playsound('BGM.mp3') print("https://www.apple.com.cn" + info_link) send_mail(info_text, "https://www.apple.com.cn" + info_link, 'XXXXXXXXX@qq.com') print('Done!') time.sleep(3) # 数字决定几秒爬取一次理论上说，这个小程序可以通知官翻网站上任何产品，比如说要买Mac，可以将url修改成Mac页面。博主拿“翻新 11 英寸 iPad Pro 无线局域网机型 64GB - 银色”为例，需要注意的是，程序的原理是爬取网页的信息，并进行查找和比对，因此想要的这个商品的名字要与官网产品的名字完全相同（包括空格）！如果你觉得邮件通知还是比较慢，并且可以在电脑前等待，你可以将 #playsound(‘BGM.mp3’)前的#号和空格去掉，那么程序在爬取到对应信息后会以播放音乐的形式通知你！（缺点是音乐不播放完不会执行下一步～）BGM博主不会提供，可以自行下载一个自己喜欢的音乐，放在python程序的根目录即可～三、完整代码分享分享代码之前，相信大家也已经看出来了，实际上能够正确运行这个程序并不容易，对于程序的种种限制也一定会深感麻烦，博主也对并非这个专业的吧友们的烦恼感到理解，希望有大神能够将程序封装成易使用的图形化界面造福吧友～如果有老哥因为我分享的程序而成功抢到心仪的产品，也希望能够打赏我，鼓励我，并且分享这篇博客给大家让更多的人看到！123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import urllib.requestimport smtplibimport timefrom playsound import playsoundfrom email.header import Headerfrom email.mime.text import MIMETextfrom bs4 import BeautifulSoupdef send_mail(title, article, receiver): host = 'smtp.qq.com' # 这是QQ邮箱SMTP服务器的host，其他邮箱有不同可具体查询 user = 'XXXXXXXXX@qq.com'#这是邮箱号 password = 'XXXXXXXXXXXXXX'#这是授权码，注意不是邮箱的密码或者QQ的密码！ sender = user coding = 'utf8' message = MIMEText(article, 'plain', coding) message['From'] = Header(sender, coding) message['To'] = Header(receiver, coding) message['subject'] = Header(title, coding) try: mail_client = smtplib.SMTP_SSL(host, 465)#部分邮箱信道不同，又有可能没有开启SSL服务，具体查询 mail_client.connect(host) mail_client.login(user, password) mail_client.sendmail(sender, receiver, message.as_string()) mail_client.close() print('邮件已成功发送给:' + receiver) except: print('发送失败!')while True: print("Get Work!") localtime = time.asctime(time.localtime(time.time())) # 报时，免得程序卡住不知道～ print(localtime) url = 'https://www.apple.com.cn/shop/refurbished/ipad' # 这个是iPad翻新页面 headers = &#123;'User-Agent': 'Mozilla/5.0 3578.98 Safari/537.36'&#125; # 添加headers防止官网认为是爬虫而屏蔽访问 req = urllib.request.Request(url, headers=headers) try: rsp = urllib.request.urlopen(req) except: continue html = rsp.read().decode('utf-8', 'ignore') # print(html) html = BeautifulSoup(html, 'html.parser') for link in html.find_all('a'): info_link = link.get('href') info_text = link.get_text(strip=True) # print(info_text) # print(info_link + '\n') if info_text == "翻新 11 英寸 iPad Pro 无线局域网机型 64GB - 银色": print('Got 翻新 11 英寸 iPad Pro 无线局域网机型 64GB - 银色!') # playsound('BGM.mp3') print("https://www.apple.com.cn" + info_link) send_mail(info_text, "https://www.apple.com.cn" + info_link, 'XXXXXXXXX@qq.com') print('Done!') time.sleep(3) # 数字决定几秒爬取一次未经允许不得转载！]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[实战MNIST手写数字识别]]></title>
    <url>%2F2019%2F04%2F12%2FPytorch-MNIST%2F</url>
    <content type="text"><![CDATA[前言读取MNIST数据集实现手写数字的识别 测试代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import torchimport numpyimport torchvisionfrom torch.autograd import Variableimport torch.utils.data.dataloader as Dataimport matplotlib.pyplot as plttrain_data = torchvision.datasets.MNIST( './mnist', train=True, transform=torchvision.transforms.ToTensor(), download=True)test_data = torchvision.datasets.MNIST( './mnist', train=False, transform=torchvision.transforms.ToTensor())print("train_data:", train_data.data.size())print("train_labels:", train_data.targets.size())print("test_data:", test_data.data.size())train_loader = Data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)test_loader = Data.DataLoader(dataset=test_data, batch_size=64)class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = torch.nn.Sequential( torch.nn.Conv2d(1, 32, 3, 1, 1), torch.nn.ReLU(), torch.nn.MaxPool2d(2)) self.conv2 = torch.nn.Sequential( torch.nn.Conv2d(32, 64, 3, 1, 1), torch.nn.ReLU(), torch.nn.MaxPool2d(2)) self.conv3 = torch.nn.Sequential( torch.nn.Conv2d(64, 64, 3, 1, 1), torch.nn.ReLU(), torch.nn.MaxPool2d(2)) self.dense = torch.nn.Sequential( torch.nn.Linear(64 * 3 * 3, 128), torch.nn.ReLU(), torch.nn.Linear(128, 10)) def forward(self, x): conv1_out = self.conv1(x) conv2_out = self.conv2(conv1_out) conv3_out = self.conv3(conv2_out) res = conv3_out.view(conv3_out.size(0), -1) out = self.dense(res) return outmodel = Net()optimizer = torch.optim.Adam(model.parameters())loss_func = torch.nn.CrossEntropyLoss()for epoch in range(1): print('epoch &#123;&#125;'.format(epoch + 1)) train_loss = 0. train_acc = 0. for batch_x, batch_y in train_loader: batch_x, batch_y = Variable(batch_x), Variable(batch_y) out = model(batch_x) loss = loss_func(out, batch_y) train_loss += loss.item() pred = torch.max(out, 1)[1] train_correct = (pred == batch_y).sum() train_acc += train_correct.item() optimizer.zero_grad() loss.backward() optimizer.step() print('Train Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format( train_loss / (len(train_data)), train_acc / (len(train_data)))) model.eval() eval_loss = 0. eval_acc = 0. for batch_x, batch_y in test_loader: with torch.no_grad(): batch_x = Variable(batch_x) with torch.no_grad(): batch_y = Variable(batch_y) out = model(batch_x) loss = loss_func(out, batch_y) eval_loss += loss.item() pred = torch.max(out, 1)[1] num_correct = (pred == batch_y).sum() eval_acc += num_correct.item() print('Test Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;'.format(eval_loss / (len(test_data)), eval_acc / (len(test_data))))data_loader_test = torch.utils.data.DataLoader(dataset=test_data, batch_size=4, shuffle=True)x_test, y_test = next(iter(data_loader_test))inputs = Variable(x_test)pred = model(inputs)_, pred = torch.max(pred, 1)print('Predict Label is:', [i for i in pred.data])print('Real Label is:', [i for i in y_test])img = torchvision.utils.make_grid(x_test)img = img.numpy().transpose(1, 2, 0)plt.imshow(img)plt.show() 测试结果12345678train_data: torch.Size([60000, 28, 28])train_labels: torch.Size([60000])test_data: torch.Size([10000, 28, 28])epoch 1Train Loss: 0.003245, Acc: 0.934217Test Loss: 0.001147, Acc: 0.973900Predict Label is: [tensor(0), tensor(2), tensor(9), tensor(4)]Real Label is: [tensor(0), tensor(2), tensor(9), tensor(4)] 测试图 代码分析实质上网络并不复杂（那么，为什么咕咕咕？）具体是由三个卷积层和一个全连接层组成，由于数据集被分成了一个个包，每个包的格式为[64, 1, 28, 28]，64是包含了64张图，1代表通道，MNIST的图像都是单通道黑白图，28是图像的长和宽。下面，我先介绍一下网络里的相关函数，再进行分析： class torch.nn.Conv2d()二维卷积函数in_channels(int) – 输入信号的通道out_channels(int) – 卷积产生的通道kernel_size(int or tuple) - 卷积核的尺寸stride(int or tuple, optional) - 卷积步长padding(int or tuple, optional) - 输入的每一条边补充0的层数常见的是这前五个参数，输入信号的通道和卷积产生的通道决定了要产生几个卷积核，kernel_size和stride是卷积核的参数，padding是由于在卷积后周围一圈的像素点会因为没有卷积而消失，因此需要人为的补充一圈像素，保持图像尺寸不变。 torch.nn.ReLU()激活函数，本质是max(0, x)，可以看出目的是实现完全抑制负数，成比例促进正数，是一种对神经元的模仿。 class torch.nn.MaxPool2d()二维池化函数kernel_size(int or tuple) - max pooling的窗口大小stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_sizepadding(int or tuple, optional) - 输入的每一条边补充0的层数最大池化比较好理解，在池化过程中保留最大值，可以说是在放大需要特征，消除不必要特征。 网络分析conv1，conv2，conv3都是由一个卷积函数，一个激励函数以及一个池化函数组成，数据集在进入conv1时的格式为[1, 28, 28]，卷积后变为[32, 28, 28]，池化后变为[32, 14, 14]，同理，在经过conv2层后格式为[64, 7, 7]，在经过conv3层后变为[64, 3, 3]；而dense层则是全连接层，目的是为了进行线性变换，使最后产生的值只有10个，这十个值可以说是模拟出来的概率，即针对0-9每一个数字的概率，值越大就代表是这个对应数字的可能性越大。这点怎么看呢？在训练过程中有一行代码是实现训练值和真实值的比较：1pred = torch.max(out, 1)[1] torch.max()第一个参数是输入Tensor，第二个参数是选择行和列，0代表比较每一列的最大值，1代表比较每一行的最大值；而max返回两个值，第一个是最大值，第二个如果选择比较行最大值，那么就返回最大值所在列，如果选择比较列最大值，那么就返回最大值所在行。后面可以指定单独返回哪个值，[0]返回最大值，[1]返回位置。代码展示的就是比较行最大值，并返回最大值所在列，我打印一些测试值数据和经过max之后的数据出来：测试值数据：123456tensor([[ -4.3027, -0.9863, 2.6202, 1.2236, -5.9825, -2.1526, -15.2012,12.2280, -2.6344, 3.1522],[ 2.3235, 2.6725, 13.7567, 2.4374, -7.5948, -7.6910, -2.6478,-6.4028, 4.3079, -6.4800],[ -2.2610, 8.7104, -0.7275, -2.9841, 1.5691, -2.3972, -1.0289,-0.6307, 0.1610, -2.7338]])max之后的数据：1tensor([7, 2, 1])可以看到测试值数据的最大值正好出现在7，2，1的位置。 深度学习真有趣（那你为什么咕咕咕？）]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[logistic回归模型]]></title>
    <url>%2F2019%2F04%2F08%2FPytorch-A-LogisticRegression-Model%2F</url>
    <content type="text"><![CDATA[前言开始涉及logistic回归，主要解决分类问题，贴一个学到的简单二分类，看书写的代码问题很大，问题比较多，踩坑踩死我…… 测试代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import torchimport numpy as npimport torch.nn as nnimport torch.optim as optimfrom torch.autograd import Variableimport matplotlib.pyplot as pltwith open('data.txt', 'r') as f: data_list = f.readlines() data_list = [i.split('\n')[0] for i in data_list] data_list = [i.split(',') for i in data_list] data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list] data = torch.Tensor(data) x_data = data[:, 0:2] y_data = data[:, 2]class LogisticRegression(nn.Module): def __init__(self): super(LogisticRegression, self).__init__() self.lr = nn.Linear(2, 1) self.sm = nn.Sigmoid() def forward(self, x): x = self.lr(x) x = self.sm(x) return xlogistic_model = LogisticRegression()loss_fn = nn.BCELoss()optimizer = optim.SGD(logistic_model.parameters(), lr=1e-3, momentum=0.9)for epoch in range(50000): x = Variable(x_data) y = Variable(y_data.unsqueeze(1)) out = logistic_model(x) loss = loss_fn(out, y) print_loss = loss.data.item() mask = out.ge(0.5).float() correct = (mask == y).sum() acc = correct.data.item() / x.size(0) optimizer.zero_grad() loss.backward() optimizer.step() if (epoch + 1) % 1000 == 0: print('*' * 10) print('epoch&#123;&#125;'.format(epoch + 1)) print('loss is:&#123;:.4f&#125;'.format(print_loss)) print('acc is:&#123;:.4f&#125;'.format(acc))x0 = list(filter(lambda x: x[-1] == 0.0, data))x1 = list(filter(lambda x: x[-1] == 1.0, data))plot_x0_0 = [i[0] for i in x0]plot_x0_1 = [i[1] for i in x0]plot_x1_0 = [i[0] for i in x1]plot_x1_1 = [i[1] for i in x1]plt.plot(plot_x0_0, plot_x0_1, 'ro', label='x_0')plt.plot(plot_x1_0, plot_x1_1, 'bo', label='x_1')plt.legend(loc='best')w0, w1 = logistic_model.lr.weight[0]w0 = w0.item()w1 = w1.item()b = logistic_model.lr.bias.item()plot_x = np.arange(30, 100, 0.1)plot_y = (-w0 * plot_x - b) / w1plt.plot(plot_x, plot_y)plt.show() 测试集12345678910111213141516171834.62365962451697,78.0246928153624,030.2867107622687,43.89499752400101,035.84740876993872,72.90219802708364,060.18259938620976,86.3855209546826,179.0327360507101,75.3443764369103,145.08327747668339,56.3163717815305,061.10666453684766,96.51142588489624,175.02474556738889,46.55401354116538,176.09878670226257,87.42056971926803,184.43281996120035,43.53339331072109,195.86155507093572,38.22527805795094,075.01365838958247,30.60326323428011,082.30705337399482,76.48196330235604,169.36458875970939,97.71869196188608,139.53833914367223,76.03681085115882,053.9710521485623,89.20735013750265,169.07014406283025,52.74046973016765,167.9468554771161746,67.857410673128,0 测试结果12345678910111213141516171819202122232425262728293031323334353637383940**********epoch5000loss is:0.5205acc is:0.8889**********epoch10000loss is:0.4586acc is:0.8889**********epoch15000loss is:0.4240acc is:0.8889**********epoch20000loss is:0.4026acc is:0.8333**********epoch25000loss is:0.3885acc is:0.7778**********epoch30000loss is:0.3786acc is:0.7778**********epoch35000loss is:0.3714acc is:0.7778**********epoch40000loss is:0.3661acc is:0.7778**********epoch45000loss is:0.3620acc is:0.7778**********epoch50000loss is:0.3589acc is:0.7778 效果图 代码分析一、sigmoid()激活函数logistic回归最核心的部分在于sigmoid激活函数，实质上这个训练模型很简单，隐藏层只有一层，再在隐藏层上面加一个sigmoid激活函数就输出到输出层了，sigmoid我放个图：在经过sigmoid激活之后所有输出值都会介于0到1之间，因为sigmoid函数在原点处变化的非常快，并且在向坐标轴两侧传播时会非常快的趋向0和1，因此作为分类问题的激活函数非常合适。 二、BCELoss()二分类交叉熵函数BCELoss()公式：在BCELoss()函数中，第一个参数必须在0到1之间，因此一般是要配合sigmoid()函数进行使用。BCELoss()函数与MSELoss()函数的区别在于BCELoss()能够区别正负数，在计算值相加平均的过程中有些误差会消失，但是MSELoss()不能区别正负数，因此误差不会消失。因此分类问题BCELoss较好。还有一点，由于sigmoid函数存在在向坐标轴两侧传播时会非常快的趋向0和1的特性，会导致梯度不明显甚至梯度消失的问题，不利于反向传播。在使用交叉熵作为损失函数后，反向传播的梯度不与sigmoid函数的导数有关，这就从一定程度上避免了梯度下降。但但但但是，在我孱弱短小的数据集下，MSELoss()函数的成绩实现了反超：12345678910111213141516171819202122232425262728293031323334353637383940**********epoch5000loss is:0.2167acc is:0.6111**********epoch10000loss is:0.1857acc is:0.8333**********epoch15000loss is:0.1663acc is:0.8889**********epoch20000loss is:0.1538acc is:0.8889**********epoch25000loss is:0.1453acc is:0.8889**********epoch30000loss is:0.1392acc is:0.8889**********epoch35000loss is:0.1348acc is:0.8889**********epoch40000loss is:0.1314acc is:0.8889**********epoch45000loss is:0.1288acc is:0.8889**********epoch50000loss is:0.1267acc is:0.8889效果图：实质上，在BCELoss()函数的训练过程中accuracy曾达到过0.8889：123456789101112**********epoch5000loss is:0.5468acc is:0.7778**********epoch10000loss is:0.4722acc is:0.8889**********epoch15000loss is:0.4320acc is:0.8889效果图但它的误差要大得多，因此这个成绩被“修正”了……具体原因我还分析不出来，估计是测试集样本太少的原因。 三、41行的ge()函数这是在python3中出现的新函数lt(a, b) 相当于 a &lt; ble(a,b) 相当于 a &lt;= beq(a,b) 相当于 a == bne(a,b) 相当于 a != bgt(a,b) 相当于 a &gt; bge(a, b)相当于 a&gt;= b41行代码的功能是如果大于0.5就返回1，反之返回0。返回值是0和1而不是bool类型。 四、偷懒不存在的matplotlib()图像我是拷贝的，具体实现不太清楚，待我仔细研究研究有空单独写一写。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多维线性回归模型]]></title>
    <url>%2F2019%2F04%2F03%2FMultivariable-Linear-Regression-Model%2F</url>
    <content type="text"><![CDATA[前言今儿个是多维线性回归模型～ 测试代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import torchimport numpy as npimport torch.nn as nnimport torch.optim as optimfrom torch.autograd import Variableimport randomimport matplotlib.pyplot as pltdef make_features(x): x = x.unsqueeze(1) return torch.cat([x**i for i in range(1, 3 + 1)], 1)W_target = torch.FloatTensor([0.5, 3, 2.4]).unsqueeze(1)b_target = torch.FloatTensor([0.9])def f(x): return x.mm(W_target) + b_target[0]def get_batch(batch_size=32, random=None): if random is None: random = torch.randn(batch_size) x = make_features(random) y = f(x) return Variable(x), Variable(y)class poly_model(nn.Module): def __init__(self): super().__init__() self.poly = nn.Linear(3, 1) def forward(self, x): out = self.poly(x) return outmodel = poly_model()criterion = nn.MSELoss()optimizer = optim.SGD(model.parameters(), lr=1e-3)epoch = 0while True: batch_x, batch_y = get_batch() output = model(batch_x) loss = criterion(output, batch_y) print_loss = loss.item() optimizer.zero_grad() loss.backward() optimizer.step() epoch += 1 if print_loss &lt; 1e-3: Breakprint("the number of epoches :", epoch)def func_format(weight, bias, n): func = '' for i in range(n, 0, -1): func += ' &#123;:.2f&#125; * x^&#123;&#125; +'.format(weight[i - 1], i) return 'y =' + func + ' &#123;:.2f&#125;'.format(bias[0])predict_weight = model.poly.weight.data.numpy().flatten()predict_bias = model.poly.bias.data.numpy().flatten()print('predicted function :', func_format(predict_weight, predict_bias, 3))real_W = W_target.numpy().flatten()real_b = b_target.numpy().flatten()print('real function :', func_format(real_W, real_b, 3))x = [random.randint(-200, 200) * 0.01 for i in range(20)]x = np.array(sorted(x))feature_x, y = get_batch(random=torch.from_numpy(x).float())y = y.data.numpy()plt.plot(x, y, 'ro', label='Original data')model.eval()x_sample = np.arange(-2, 2, 0.01)x, y = get_batch(random=torch.from_numpy(x_sample).float())y = model(x)y_sample = y.data.numpy()plt.plot(x_sample, y_sample, label='Fitting Line')plt.show() 测试结果123the number of epoches : 1727predicted function : y = 2.41 * x^3 + 2.99 * x^2 + 0.45 * x^1 + 0.93real function : y = 2.40 * x^3 + 3.00 * x^2 + 0.50 * x^1 + 0.90 效果图 这个效果图在matplotlib中的实现要比一维线性回归模型要难一些，具体是：80-84行先随机给一些在拟合方程上的点集，命名为’Original data’；86-92行描绘出预测曲线，可以比较直观的看重合度。预测曲线在x轴上位于-2到2之间，x轴上的各点之间相差0.01，主要是为了曲线更加平滑。 相关函数一、unsqueeze()这里先介绍squeeze()函数，squeeze()中的参数0、1分别代表第零、第一维度，也就是行和列，理所应当的，squeeze(0)表示如果第零维度值为1，则去掉，否则不变。我写个例子：123456import torchimport numpy as npa = torch.randn(3, 1)print(a)a = a.squeeze(1)print(a)结果是：1234tensor([[-0.2699],[ 0.3355],[-0.3069]])tensor([-0.2699, 0.3355, -0.3069])我感觉就是差不多行列变换，在行或者列为1的情况下。unsqueeze()就是反向操作，咋变过来的就咋变回去：12345678import torchimport numpy as npa = torch.randn(3, 1)print(a)a = a.squeeze(1)print(a)a = a.unsqueeze(1)print(a)结果是：1234567tensor([[-0.4933],[ 0.0155],[-0.5852]])tensor([-0.4933, 0.0155, -0.5852])tensor([[-0.4933],[ 0.0155],[-0.5852]]) 二、torch.cat()torch.cat()就是讲Tensor拼接在一起，有没有想到C里的strcat()，差不多～cat全称是concatnatetorch.cat()放两个参数，第一个是放需要拼接的Tensor，可以这样：1C=torch.cat((A,B),0)比较常见，也可以是测试代码里的这样：1torch.cat([x**i for i in range(1, 3 + 1)], 1)这里是通过循环的形式拼接三个Tensor。第二个参数是横向或者纵向拼接，事实上还是按照第零维度或者第一维度比较好理解：0代表行与行拼接，1代表列与列拼接。 代码分析具体思路和一维线性回归差不多，就是前期前向传播构成计算图是函数与函数之间相互嵌套比较复杂。这个我发现一个以前没有注意到的点：1x.mm(W_target) + b_target[0]其中x为32行3列，W_target为3行1列，b_target只有一个元素，照道理矩阵加法不能成立，但是：123456789101112import torchimport numpyx = torch.ones(3)x = x.unsqueeze(1)print(x)x = torch.cat([x**i for i in range(1, 4)], 1)W_target = torch.FloatTensor([0.5, 3, 2.4]).unsqueeze(1)b_target = torch.FloatTensor([0.9])y = x.mm(W_target)print(y)y += b_target[0]print(y)结果为：123456789tensor([[1.],[1.],[1.]])tensor([[5.9000],[5.9000],[5.9000]])tensor([[6.8000],[6.8000],[6.8000]])实际上是每行都加了b！比较神奇~]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一维线性回归模型]]></title>
    <url>%2F2019%2F04%2F02%2FOne-Dimensional-Linear-Regression-Model%2F</url>
    <content type="text"><![CDATA[前言本来今天是开始读取MNIST数据集完成手写数字识别的，但是遇到很大问题，MNIST的图像通道不一样导致运行错误。所以转向简单一点的线性模型来做做看。 测试代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import torchimport numpy as npimport torch.nn as nnimport torch.optim as optimimport matplotlib.pyplot as pltfrom torch.autograd import Variablex_train = np.array( [[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], [9.779], [6.182], [7.59], [2.167], [7.042], [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)y_train = np.array( [[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366], [2.596], [2.53], [1.221], [2.827], [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)x_train = torch.from_numpy(x_train)y_train = torch.from_numpy(y_train)class LinearRegression(nn.Module): def __init__(self): super(LinearRegression, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): out = self.linear(x) return outmodel = LinearRegression()loss_fn = nn.MSELoss()optimizer = optim.SGD(model.parameters(), lr=1e-3)epoch_n = 1000for epoch in range(epoch_n): inputs = Variable(x_train) target = Variable(y_train) out = model(inputs) loss = loss_fn(out, target) optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 100 == 0: print("Epoch[&#123;&#125;/&#123;&#125;], Loss:&#123;:.6f&#125;".format(epoch, epoch_n, loss.item())model.eval()predict = model(Variable(x_train))predict = predict.data.numpy()plt.plot(x_train.numpy(), y_train.numpy(), "ro", label="Original Data")plt.plot(x_train.numpy(), predict, label="Fitting Line")plt.show() 测试结果12345678910Epoch[0/1000], Loss:1.168603Epoch[100/1000], Loss:0.179088Epoch[200/1000], Loss:0.178578Epoch[300/1000], Loss:0.178095Epoch[400/1000], Loss:0.177635Epoch[500/1000], Loss:0.177198Epoch[600/1000], Loss:0.176784Epoch[700/1000], Loss:0.176390Epoch[800/1000], Loss:0.176015Epoch[900/1000], Loss:0.175660 matplotlib图像 代码分析先给几个横坐标和预期纵坐标，然后将numpy的ndarray形式转换成torch的tensor形式。接着定义一个类继承自torch.nn.Module，用于前向传播，init()和forward()两个函数是自定义类的主要函数。在init()中添加一句super(LinearRegression, self).init()用于继承父类的初始化函数。init()中主要是对神经网络的模块进行声明，具体函数实现则是在forward()中。自定义类中的成员都通过self指针来进行访问，所以参数列表中都包含了self。在这里误差函数还是使用MSELoss()用于计算均方误差，因为有的点可能在直线下面，我们不允许有负数存在；优化函数则使用SGD()，具体不知道为什么，但是换成Adam()运行效果没有SGD()好。开始训练时要把节点用Variable()纳入计算图中，不然计算不了梯度。optimizer.zero_grad()用于梯度置零，否则会造成结果不收敛。loss.backward()计算梯度。optimizer.step()更新梯度。 打印结果后可以整一个matplotlib的图出来看看。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[走向自动化：神经网络训练模型加入torch.nn和torch.optim]]></title>
    <url>%2F2019%2F04%2F01%2FPytorch-A-Simple-Neural-Networks-With-torch-nn-and-torch-optim%2F</url>
    <content type="text"><![CDATA[前言本文将尝试在上一篇博客的训练模型的基础上加入torch.nn包以及torch.optim包来优化我的训练模型，使其在更少的训练次数里得到更好的优化结果。 一、torch.nn包Pytorch中torch.nn包提供了很多实现神经网络的具体功能的类，具体本文就不展开讲了，以后肯定会涉及到，本文主要介绍在训练模型里用到的相关函数。测试代码12345678910111213141516171819202122232425262728293031import torchfrom torch.autograd import Variablebatch_n = 100hidden_layer = 100input_data = 1000output_data = 10x = Variable(torch.randn(batch_n, input_data), requires_grad=False)y = Variable(torch.randn(batch_n, output_data), requires_grad=False)models = torch.nn.Sequential( torch.nn.Linear(input_data, hidden_layer), torch.nn.ReLU(), torch.nn.Linear(hidden_layer, output_data))epoch_n = 10000learning_rate = 1e-4loss_fn = torch.nn.MSELoss()for epoch in range(epoch_n): y_pred = models(x) loss = loss_fn(y_pred, y) if epoch % 1000 == 0: print("Epoch:&#123;&#125;, Loss:&#123;:.4f&#125;".format(epoch, loss.item())) models.zero_grad() loss.backward() for param in models.parameters(): param.data -= param.grad.data * learning_rate测试结果12345678910Epoch:0, Loss:0.9800Epoch:1000, Loss:0.9122Epoch:2000, Loss:0.8534Epoch:3000, Loss:0.8013Epoch:4000, Loss:0.7544Epoch:5000, Loss:0.7119Epoch:6000, Loss:0.6727Epoch:7000, Loss:0.6366Epoch:8000, Loss:0.6031Epoch:9000, Loss:0.5715 1.torch.nn.SequentialSequential类是一种序列容器，通过在容器中嵌套各种实现神经网络中具体功能的类，来完成神经网络模型的搭建。如果说上一篇博客提到的Variable类是将前向传播中的计算过程变成一张计算图，那么Sequential类就是将计算图封装在容器里，更为特殊的是，计算图的每一个步骤在容器里都有自己的名字，因此每一个步骤都可以当作模块单独拿出来调用。模块的命名方式可以是按照数字序列从零开始命名，也可以import一个OrderedDict包以有序字典的形式命名，这种方法的话对每个模块可以自己命名，有点类似于python列表和字典的区别。例如：1234567from collections import OrderedDictmodels = torch.nn.Sequential( OrderedDict([ ("Line1", torch.nn.Linear(input_data, hidden_layer)), ("ReLU1", torch.nn.ReLU()), ("Line2", torch.nn.Linear(hidden_layer, output_data))])) 2.torch.nn.Lineartorch.nn.Linear用于定义模型的线性层，即完成不同层之间的线性变换（如模型中输入层和隐藏层以及隐藏层和输出层的线性变换）。Linear有三个参数，分别是输入特征数，输出特征数以及是否使用偏置（默认为True）。Linear会自动生成权重参数和偏置（默认情况），因此在模型中不需要单独定义如w1w2之类的权重参数，并且Linear提供比原先自定义权重参数时使用的randn随机正太分布更好的参数初始化方法，让人放心～ 3.torch.nn.ReLU一看名字就是激活函数，先挖一个坑，以后一起讲，不讲可以打我，话放在这里了。 4.仨常见损失函数torch.nn.MSELoss：均方误差函数torch.nn.L1Loss：平均绝对误差函数torch.nn.CrossEntropyLoss：交叉熵函数仨在定义类的对象时都不需要传参，只要在使用实例时输入维度一样的参数即可计算（交叉熵函数要满足交叉熵的计算条件） 5.torch.nn.parameter有点搞不明白，所以看了一下官方的docs：12345678910111213141516r"""A kind of Tensor that is to be considered a module parameter.Parameters are :class:`~torch.Tensor` subclasses, that have avery special property when used with :class:`Module` s - when they'reassigned as Module attributes they are automatically added to the list ofits parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.Assigning a Tensor doesn't have such effect. This is because one mightwant to cache some temporary state, like last hidden state of the RNN, inthe model. If there was no such class as :class:`Parameter`, thesetemporaries would get registered too.Arguments:data (Tensor): parameter tensor.requires_grad (bool, optional): if the parameter requires gradient. See:ref:`excluding-subgraphs` for more details. Default: `True`"""看着像是一个迭代器，也就是说在使用torch.nn包中的类进行神经网络的搭建之后，网络的参数都会保存在parameters()函数当中，访问models中的参数是对models.parameters()进行遍历完成的，然后才对每个遍历的参数进行更新。 6.???我一开始在看代码的时候发现1models.zero_grad()这个梯度置零的函数放的位置不太对，然后试着把它注释掉，结果一运行发现：12345678910Epoch:0, Loss:1.0549Epoch:1000, Loss:0.2391Epoch:2000, Loss:0.2478Epoch:3000, Loss:0.1410Epoch:4000, Loss:0.1171Epoch:5000, Loss:0.0837Epoch:6000, Loss:0.0743Epoch:7000, Loss:0.0634Epoch:8000, Loss:0.0581Epoch:9000, Loss:0.0590误差降低了十倍？这超出了我的知识范围……希望以后有能力解答这个问题…… 二、torch.optim我的训练模型到现在仍在自定义学习速率，这是因为输出层只有一个比较简单导致的。如果网络一复杂仍然自定义学习速率来进行权重参数的更新是非常不现实的，torch.optim包中正好提供了很多自动优化的类，下面介绍其中的Adam类。 测试代码1234567891011121314151617181920212223242526272829303132import torchfrom torch.autograd import Variablebatch_n = 100hidden_layer = 100input_data = 1000output_data = 10x = Variable(torch.randn(batch_n, input_data), requires_grad=False)y = Variable(torch.randn(batch_n, output_data), requires_grad=False)models = torch.nn.Sequential( torch.nn.Linear(input_data, hidden_layer), torch.nn.ReLU(), torch.nn.Linear(hidden_layer, output_data))epoch_n = 100learning_rate = 1e-4loss_fn = torch.nn.MSELoss()optimizer = torch.optim.Adam(models.parameters(), lr=learning_rate)for epoch in range(epoch_n): y_pred = models(x) loss = loss_fn(y_pred, y) if epoch % 10 == 0: print("Epoch:&#123;&#125;, Loss:&#123;:.4f&#125;".format(epoch, loss.item())) optimizer.zero_grad() loss.backward() optimizer.step()测试结果12345678910Epoch:0, Loss:1.0926Epoch:10, Loss:0.8934Epoch:20, Loss:0.7386Epoch:30, Loss:0.6166Epoch:40, Loss:0.5177Epoch:50, Loss:0.4355Epoch:60, Loss:0.3654Epoch:70, Loss:0.3050Epoch:80, Loss:0.2525Epoch:90, Loss:0.2068仅仅100次就达到了比优化前10000次更好的效果 1.torch.optim.Adamtorch.optim.Adam类有两个参数，分别是需要被优化的参数和学习速率（默认为1e-2）。Adam的表现之所以这么优秀，是因为它可以做到使学习速率自适应调节，达到最好的速率。加入优化算法，每次训练的梯度更新可以写成1optimizer.step() 2.???注释掉梯度置零函数1optimizer.zero_grad()好像训练效果又好了那么一点点……]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[简单训练模型加入autograd实现自动梯度计算]]></title>
    <url>%2F2019%2F03%2F31%2FPytorch-A-Simple-Neural-Networks-With-Autograd%2F</url>
    <content type="text"><![CDATA[前言今天的训练模型将在昨天的训练模型基础上做出改进，通过Pytorch中的autograd包替代后向传播中的手动链式求导，实现自动梯度功能。（昨天的后向传播部分写的非常简略，主要是里面有个.t()函数一直查不出来是干啥用的……感觉自己理解的也非常模糊，只知道是在求偏导数……幸好有autograd可以掩盖我的无知……） 测试代码1234567891011121314151617181920212223242526272829import torchfrom torch.autograd import Variablebatch_n = 100hidden_layer = 100input_data = 1000output_data = 10x = Variable(torch.randn(batch_n, input_data), requires_grad=False)y = Variable(torch.randn(batch_n, output_data), requires_grad=False)w1 = Variable(torch.randn(input_data, hidden_layer), requires_grad=True)w2 = Variable(torch.randn(hidden_layer, output_data), requires_grad=True)epoch_n = 20learning_rate = 1e-6for epoch in range(epoch_n): y_pred = x.mm(w1).clamp(min=0).mm(w2) loss = (y_pred - y).pow(2).sum() print("Epoch:&#123;&#125;, Loss:&#123;:.4f&#125;".format(epoch, loss.data)) loss.backward() w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data w1.grad.data.zero_() w2.grad.data.zero_() 运行结果：1234567891011121314151617181920Epoch:0, Loss:46125084.0000Epoch:1, Loss:97735776.0000Epoch:2, Loss:353886400.0000Epoch:3, Loss:681477440.0000Epoch:4, Loss:70418120.0000Epoch:5, Loss:19572480.0000Epoch:6, Loss:10264789.0000Epoch:7, Loss:6297184.5000Epoch:8, Loss:4209952.5000Epoch:9, Loss:2988804.0000Epoch:10, Loss:2225728.0000Epoch:11, Loss:1726369.1250Epoch:12, Loss:1387307.2500Epoch:13, Loss:1149427.6250Epoch:14, Loss:977541.5625Epoch:15, Loss:849593.3125Epoch:16, Loss:751535.7500Epoch:17, Loss:674315.6875Epoch:18, Loss:611959.5625Epoch:19, Loss:560407.1250由于在定义输入层和输出层时使用的是randn随即正态分布函数，运行结果会有很大不同，我挑了一组比较明显的结果。 代码分析torch.grad包u实现自动梯度的过程大致为先通过定义的Tensor类型数据变量在前向传播中生成计算图，然后根据计算图和输出结果计算出每个参数需要更新的梯度，并通过后向传播完成对参数的梯度更新。开头需要导入torch.grad包中的Variable类从而对Tensor数据类型变量进行封装。在计算图中，每个节点都应该是Variable类型的对象，这样才能应用自动梯度的功能。requires_grad一看名字就知道，决定了Variable类型是否需要计算梯度，w1、w2是权重参数，所以为True。.backward()函数根据我们自己设置的需求求出梯度值并保留，再结合学习速率对权重参数进行更新，最后通过.zero_()函数对grad.data进行置零，否则梯度值会一直累加，影响后边儿计算。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[入门神经网络训练模型搭建]]></title>
    <url>%2F2019%2F03%2F30%2FPytorch-A-Simple-Neural-Networks%2F</url>
    <content type="text"><![CDATA[前言今天试着搭建一个小的神经网络训练模型，通过训练会使预测值和真实值逐渐逼近。 测试代码123456789101112131415161718192021222324252627282930313233343536373839404142import torch#一次输入数据的数量 即100组数据batch_n = 100#每组数据的数据特征数input_data = 1000#数据经过隐藏层后保留的数据特征的个数（只考虑一层隐藏层）hidden_layer = 100#每组输出数据的数据特征数output_data = 10#输入层和输出层x = torch.randn(batch_n, input_data)y = torch.randn(batch_n, output_data)#输入层到隐藏层的权重参数和隐藏层到输出层的权重参数w1 = torch.randn(input_data, hidden_layer)w2 = torch.randn(hidden_layer, output_data)#训练次数epoch_n = 20#学习速率learning_rate = 1e-6for epoch in range(epoch_n): h1 = x.mm(w1) h1 = h1.clamp(min=0) y_pred = h1.mm(w2) loss = (y_pred - y).pow(2).sum() print("Epoch:&#123;&#125;, Loss:&#123;:.4f&#125;".format(epoch, loss)) grad_y_pred = 2 * (y_pred - y) grad_w2 = h1.t().mm(grad_y_pred) grad_h = grad_y_pred.clone() grad_h = grad_h.mm(w2.t()) grad_h.clamp_(min=0) grad_w1 = x.t().mm(grad_h) w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 测试结果：1234567891011121314151617181920Epoch:0, Loss:58291408.0000Epoch:1, Loss:159056992.0000Epoch:2, Loss:521834688.0000Epoch:3, Loss:522471264.0000Epoch:4, Loss:4316087.5000Epoch:5, Loss:3243381.0000Epoch:6, Loss:2620373.0000Epoch:7, Loss:2226943.0000Epoch:8, Loss:1957315.6250Epoch:9, Loss:1758743.6250Epoch:10, Loss:1603234.8750Epoch:11, Loss:1475709.2500Epoch:12, Loss:1367604.5000Epoch:13, Loss:1273678.0000Epoch:14, Loss:1190754.3750Epoch:15, Loss:1116698.3750Epoch:16, Loss:1049910.6250Epoch:17, Loss:989398.0625Epoch:18, Loss:934201.0625Epoch:19, Loss:883768.8750 一、前向传播26-28行其实就是用输入层进入隐藏层再进入输出层的整个过程了，通过mm函数（注意不是mul函数）实现了数据特征从1000到100到10的变化，在计算过程中还对矩阵的乘积使用clamp函数进行裁剪，将小于0的数赋值为0（clamp函数本来有3个参数，分别是需要进行裁剪的Tensor变量，裁剪的上边界和下边界，在这里需要裁剪的变量是其本身，只定义下边界不定义上边界是为了实现ReLU激活函数的功能，但并不完全等于ReLU激活函数）30-31行计算损失函数并打印，这里使用的是均方误差的计算方法，给个图：求出来是真实值和预测值的差值的平方和。 二、后向传播后向传播主要是为了对权重值w进行优化。33-39行grad_w1和grad_w2是更新的权重梯度值。（梯度就是多元函数的偏导数以向量形式表示）41-42行在得到参数的梯度值之后，按照设置好的学习速率对权重值进行更新。]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基本是最终版本的小工程]]></title>
    <url>%2F2019%2F03%2F27%2FOpenCV-Learning-Day-17%2F</url>
    <content type="text"><![CDATA[前言昨天角度定位的问题解决了……角度还是算不出来，我调整了几个函数，在图像旋转之后再次使用Rotate()函数得到外接矩形的点阵信息，然后标记ROI图形区域裁剪图像并用imwrite()函数读出。 基本是最终版本的测试代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;cmath&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;float angle;Point2f rect[4];void minRect(Mat&amp; srcImage);void Rotate(Mat &amp;src,Mat &amp;dst,float angle);void drawLine(Mat&amp; img);void cutImage(Mat&amp; img);void saveImage(Mat&amp; img,int i);void makeBorder(Mat&amp; src,Mat&amp; dst,int B,int G,int R);static inline bool ContoursSortFunction(vector&lt;Point&gt; contour1,vector&lt;Point&gt; contour2);int main()&#123; string path = "/Users/cezarbao/Desktop/TestImages/*.bmp"; vector&lt;Mat&gt;images; vector&lt;String&gt;srcImages; glob(path,srcImages,false); size_t cnt = srcImages.size(); for(int i = 0; i &lt; cnt; i++) &#123; images.push_back(imread(srcImages[i])); makeBorder(images[i],images[i],255,255,255); minRect(images[i]); Rotate(images[i],images[i],angle); minRect(images[i]); //drawLine(images[i]); cutImage(images[i]); saveImage(images[i],i); imshow("image",images[i]); waitKey(0); &#125;&#125;void minRect(Mat&amp; srcImg)&#123; Mat dstImage = srcImg.clone(); cvtColor(dstImage, dstImage, COLOR_BGR2GRAY); threshold(dstImage, dstImage, 254, 255, THRESH_BINARY); //imshow("srcImage", dstImage); vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarcy; findContours(dstImage, contours, hierarcy, RETR_LIST, CHAIN_APPROX_NONE); sort(contours.begin(),contours.end(),ContoursSortFunction); contours.erase(contours.begin()); vector&lt;RotatedRect&gt; box(contours.size()); for(int i = 0; i &lt; 1; i++) &#123; box[i] = minAreaRect(Mat(contours[i])); box[i].points(rect); angle = box[i].angle; //for(int j = 0; j &lt; 4; j++) //&#123; // cout &lt;&lt; rect[j] &lt;&lt; endl; //&#125; &#125;&#125;static inline bool ContoursSortFunction(vector&lt;Point&gt; contour1,vector&lt;Point&gt; contour2)&#123; return (contourArea(contour1) &gt; contourArea(contour2));&#125;void Rotate(Mat &amp;src,Mat &amp;dst,float angle)&#123; makeBorder(src,dst,0,0,0); //rotate Point2f center( (float)(dst.cols/2) , (float) (dst.rows/2)); Mat affine_matrix = getRotationMatrix2D( center, angle, 1.0 ); warpAffine(dst, dst, affine_matrix, dst.size());&#125;void drawLine(Mat&amp; img)&#123; for(int i=0; i&lt;4; i++) &#123; line(img, rect[i], rect[(i+1)%4], Scalar(0, 0, 255),2,LINE_AA); &#125;&#125;void cutImage(Mat&amp; img)&#123; int width = rect[2].x - rect[1].x; int height = rect[0].y - rect[1].y; Mat ROI = img(Rect(rect[1].x,rect[1].y,width,height)); ROI.copyTo(img);&#125;void saveImage(Mat&amp; img,int i)&#123; char path[256] = &#123;0&#125;; sprintf(path,"./outputImages/%d.jpg",i); imwrite(path,img);&#125;void makeBorder(Mat&amp; src,Mat&amp; dst,int B,int G,int R)&#123; int maxBorder =(int) (max(src.cols, src.rows)* 1.414); int dx = (maxBorder - src.cols)/2; int dy = (maxBorder - src.rows)/2; copyMakeBorder(src, dst, dy, dy, dx, dx, BORDER_CONSTANT,Scalar(B,G,R));&#125; 后记测试图没啥好看的，老师发的零件图片，效果图也不贴了（不是我懒）并无新函数，但就这玩意儿我写了三天……后面算是整理的比较清楚了，功能基本上都写在函数里了，就酱。开始学习Pytorch。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[warpAffine()函数实现最小外接矩形和原始图像平行]]></title>
    <url>%2F2019%2F03%2F26%2FOpenCV-Learning-Day-16%2F</url>
    <content type="text"><![CDATA[前言本来今天想一并吧图像的旋转和裁剪给一起完成了，但是图像在旋转之后的坐标定位一直有问题，所以今天只好放出在昨天的博客的基础上做的图像旋转的函数，旋转之后目标图像的最小外接矩形是和原始图像平行的，为的是设置ROI为裁剪图像做准备。 测试代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;cmath&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;float angle;Point pointlu;void minRect(Mat&amp; srcImage);void Rotate(Mat &amp;src,Mat &amp;dst,float angle);static inline bool ContoursSortFunction(vector&lt;Point&gt; contour1,vector&lt;Point&gt; contour2);int main()&#123; string path = "/Users/cezarbao/Desktop/TestImages/*.bmp"; vector&lt;Mat&gt;images; vector&lt;String&gt;srcImages; glob(path,srcImages,false); size_t cnt = srcImages.size(); for(int i = 0; i &lt; cnt; i++) &#123; images.push_back(imread(srcImages[i])); minRect(images[i]); Rotate(images[i],images[i],angle); imshow("image",images[i]); waitKey(0); &#125;&#125;void minRect(Mat&amp; srcImg)&#123; Mat dstImage = srcImg.clone(); cvtColor(dstImage, dstImage, COLOR_BGR2GRAY); threshold(dstImage, dstImage, 254, 255, THRESH_BINARY); //imshow("srcImage", dstImage); vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarcy; findContours(dstImage, contours, hierarcy, RETR_LIST, CHAIN_APPROX_NONE); sort(contours.begin(),contours.end(),ContoursSortFunction); contours.erase(contours.begin()); vector&lt;RotatedRect&gt; box(contours.size()); //定义最小外接矩形集合 Point2f rect[4]; for(int i=0; i &lt; 1; i++) &#123; box[i] = minAreaRect(Mat(contours[i])); //计算每个轮廓最小外接矩形 box[i].points(rect); //把最小外接矩形四个端点复制给rect数组 angle = box[i].angle; for(int j=0; j&lt;4; j++) &#123; line(srcImg, rect[j], rect[(j+1)%4], Scalar(0, 0, 255)); //绘制最小外接矩形每条边 &#125; &#125;&#125;static inline bool ContoursSortFunction(vector&lt;Point&gt; contour1,vector&lt;Point&gt; contour2)&#123; return (contourArea(contour1) &gt; contourArea(contour2));&#125;void Rotate(Mat &amp;src,Mat &amp;dst,float angle)&#123; //fill int maxBorder =(int) (max(src.cols, src.rows)* 1.414); //即为sqrt(2)*max int dx = (maxBorder - src.cols)/2; int dy = (maxBorder - src.rows)/2; copyMakeBorder(src, dst, dy, dy, dx, dx, BORDER_CONSTANT,Scalar(0,0,0)); //rotate Point2f center( (float)(dst.cols/2) , (float) (dst.rows/2)); Mat affine_matrix = getRotationMatrix2D( center, angle, 1.0 );//求得旋转矩阵 warpAffine(dst, dst, affine_matrix, dst.size());&#125; 一、copyMakeBorder()边缘扩充函数函数原型：12345void copyMakeBorder( const Mat&amp; src, Mat&amp; dst, int top, int bottom, int left, int right, int borderType, const Scalar&amp; value=Scalar())第一个参数和第二个参数分别是输入图像和输出图像top、bottom、left、right参数非常明显，分别是顶端、底部、左边和右边所需填充的距离borderType：扩充边缘的类型，就是外插的类型，OpenCV中给出以下几种方式BORDER_REPLICATE 对边界像素进行复制BORDER_REFLECT 对感兴趣的图像中的像素在两边分别进行复制，也就是左右各复制一次BORDER_REFLECT_101 只复制一次，左右各占一半BORDER_WRAP 外包装（？）BORDER_CONSTANT 常量，选择这种方式的时候，需要用到最后一个参数为扩充的边缘选择颜色 二、warpAffine()旋转函数函数原型：123456789void warpAffine( InputArray src, OutputArray dst, InputArray M, Size dsize, int flags = INTER_LINEAR, int borderMode = BORDER_CONSTANT, const Scalar &amp; borderValue = Scalar() )src: 输入图像dst: 输出图像，尺寸由dsize指定，图像类型与原图像一致M: 2X3的变换矩阵dsize: 指定图像输出尺寸flags: 插值算法标识符，有默认值INTER_LINEARborderMode: 边界像素模式，有默认值BORDER_CONSTANTborderValue: 边界取值，有默认值Scalar()即0 三、getRotationMatrix2D()旋转图像矩阵取得函数warpAffine()函数的InputArrey M的取得需要用到这个函数函数原型：12345Mat getRotationMatrix2D( Point2f center, double angle, double scale)center: Point2f类型，表示原图像的旋转中心angle: double类型，表示图像旋转角度，角度为正则表示逆时针旋转，角度为负表示逆时针旋转（坐标原点是图像左上角）scale: 缩放系数]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[最小外接矩形（不完美）]]></title>
    <url>%2F2019%2F03%2F25%2FOpenCV-Learning-Day-15%2F</url>
    <content type="text"><![CDATA[前言本章节结合前面一章的批处理函数glob()，进行图像的最小外接矩形的描绘（实际效果不太完美）。 测试代码：12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;void minRect(Mat&amp; srcImage);int main()&#123; string path = "/Users/cezarbao/Desktop/DeepLearning/OpenCV/*.jpeg"; vector&lt;Mat&gt;images; vector&lt;String&gt;srcImages; glob(path,srcImages,false); size_t cnt = srcImages.size(); for(int i = 0; i &lt; cnt; i++) &#123; images.push_back(imread(srcImages[i])); minRect(images[i]); &#125;&#125;void minRect(Mat&amp; srcImg)&#123; Mat dstImage = srcImg.clone(); cvtColor(srcImg, srcImg, COLOR_BGR2GRAY); threshold(srcImg, srcImg, 100, 255, THRESH_BINARY); vector&lt;vector&lt;Point&gt;&gt; contours; vector&lt;Vec4i&gt; hierarcy; findContours(srcImg, contours, hierarcy, RETR_EXTERNAL, CHAIN_APPROX_NONE); vector&lt;RotatedRect&gt; box(contours.size()); //定义最小外接矩形集合 Point2f rect[4]; for(int i=0; i&lt;contours.size(); i++) &#123; box[i] = minAreaRect(Mat(contours[i])); //计算每个轮廓最小外接矩形 box[i].points(rect); //把最小外接矩形四个端点复制给rect数组 for(int j=0; j&lt;4; j++) &#123; line(dstImage, rect[j], rect[(j+1)%4], Scalar(0, 0, 255), 2, LINE_AA); //绘制最小外接矩形每条边 &#125; &#125; imshow("Image",dstImage); waitKey(0);&#125;效果图：（可以看到第二张图明显识别出了问题……） 一、findContours()轮廓扫描函数函数原型：12345678findContours( InputOutputArray image, OutputArrayOfArrays contours, OutputArray hierarchy, int mode, int method, Point offset=Point())第一个参数是输入图像，这里有很大一个坑，输入图像必须是八位单通道图像（即8UC1），在函数中被认为是一个二值化图像（即所有非零元素都被视作是相等的，非0即1），但如果mode是CV_RETR_CCOMP 或者 CV_RETR_FLOODFILL，输入图像也可以是32位的整型图像(CV_32SC1)。第二个参数是二维vector数组，这里将使用找到的轮廓的列表进行填充（即，这将是一个contours的vector,其中contours[i]表示一个特定的轮廓，这样，contours[i][j]将表示contour[i]的一个特定的端点）。第三个参数可以指定，也可以不提指定。如果指定的话，输出hierarchy，将会描述输出轮廓树的结构信息。（Vec4i是Vec&lt;int,4&gt;的别名，定义了一个“向量内每一个元素包含了4个int型变量”的向量）第四个参数将会告诉OpenCV你想用何种方式来对轮廓进行提取RETR_EXTERNAL：表示只提取最外面的轮廓；RETR_LIST：表示提取所有轮廓并将其放入列表；RETR_CCOMP:表示提取所有轮廓并将组织成一个两层结构，其中顶层轮廓是外部轮廓，第二层轮廓是“洞”的轮廓；RETR_TREE：表示提取所有轮廓并组织成轮廓嵌套的完整层级结构。第五个参数给出轮廓如何呈现的方法CHAIN_APPROX_NONE：将轮廓中的所有点的编码转换成点；CHAIN_APPROX_SIMPLE：压缩水平、垂直和对角直线段，仅保留它们的端点； CHAIN_APPROX_TC89_L1 or CHAIN_APPROX_TC89_KCOS：应用Teh-Chin链近似算法中的一种风格 二、minAreaRect()函数函数原型：1RotatedRect minAreaRect(InputArray points)第一个参数可以输入点阵容器（vector）或者Mat类型的图像，这里很坑的是，如果参数是Mat类型必须满足depth == CV_32F || depth == CV_32S，且checkVector(2)才可以，否则会报错（minAreaRect()中主要调用的求凸包的函数convexHull()会检查Mat满不满足上面的条件）。Mat::depth()函数：求矩阵中元素的一个通道的数据类型，这个值和type是相关的。Mat::checkVector()函数：当Mat的channels,depth,和连续性 满足checkVector的参数内容时,返回(int)(total()*channels()/_elemChannels), 否则返回-1。checkVector(2)，要求矩阵的列数位2。（我就是因为minAreaRect()中的Mat类总是报错才怒转findContours()函数的，然而测试代码的视线效果并不完美，有待改进） 三、RotatedRect类RotatedRect类常用于配合minAreaRect()函数的计算（因为minAreaRect()函数的返回类型就是RotatedRect类）函数定义：1234567891011121314151617class CV_EXPORTS RotatedRect&#123; public: //! various constructors RotatedRect(); RotatedRect(const Point2f&amp; center, const Size2f&amp; size, float angle); RotatedRect(const CvBox2D&amp; box); //! returns 4 vertices of the rectangle void points(Point2f pts[]) const; //! returns the minimal up-right rectangle containing the rotated rectangle Rect boundingRect() const; //! conversion to the old-style CvBox2D structure operator CvBox2D() const; Point2f center; //&lt; the rectangle mass center Size2f size; //&lt; width and height of the rectangle float angle; //&lt; the rotation angle. When the angle is 0, 90, 180, 270 etc., the rectangle becomes an up-right rectangle.&#125;;RotatedRect类中定义了矩形的中心点center、尺寸size（包括width、height）、旋转角度angle共3个成员变量；points()函数用于求矩形的4个顶点，boundingRect()函数求包含最小外接矩形的，与坐标轴平行（或垂直）的最小矩形。参考博文：OpenCV 中boundingRect、minAreaRect的用法区别blog.csdn.net/u013925378/article/details/84563011Opencv轮廓检测findContours分析（层次结构）www.jianshu.com/p/4bc3349b4611【OpenCV3】图像轮廓查找与绘制——cv::findContours()与cv::drawContours()详解blog.csdn.net/guduruyu/article/details/69220296Opencv RotatedRect类中的points、angle、width、height等详解blog.csdn.net/mailzst1/article/details/83141632]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[批处理函数和二值化函数]]></title>
    <url>%2F2019%2F03%2F24%2FOpenCV-Learning-Day-14%2F</url>
    <content type="text"><![CDATA[前言图像批处理，管进不管出。 测试代码：1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;string&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;int main()&#123; string path = "/Users/cezarbao/Desktop/TestImages/*.bmp"; vector&lt;Mat&gt;images; vector&lt;String&gt;srcImages; glob(path,srcImages,false); size_t cnt = srcImages.size(); for(int i = 0; i &lt; cnt; i++) &#123; images.push_back(imread(srcImages[i])); //cvtColor(images[i],images[i],COLOR_BGR2GRAY); //blur(images[i],images[i],Size(5,5)); threshold(images[i],images[i],170,255,THRESH_BINARY); imshow("Image",images[i]); waitKey(0); &#125;&#125; 一、glob()批处理函数glob()函数可以批处理指定文件夹中的图像，函数目的是将pattern路径下的所用文件名存进&amp;result中函数原型：1void glob(String pattern, vector&lt;String&gt; &amp;result, bool recursive = false)第一个参数是文件夹的绝对路径第二个参数是存放图片名称和路径的vector容器，需要注意的是该容器必须是OpenCV的String类第三个参数默认是false，当recursive为false时，仅仅遍历指定文件夹内符合模式的文件，当recursive为true时，会同时遍历指定文件夹的子文件夹 二、threshold()二值化函数函数原型：1234567double threshold( InputArray src, OutputArray dst, double thresh, double maxval, int type)第一个参数和第二个参数分别为输入图像和输出图像thresh参数表示阈值maxval参数表示与THRESH_BINARY和THRESH_BINARY_INV阈值类型一起使用设置的最大值。type参数表示阈值类型：12345type=CV_THRESH_BINARY //如果 src(x,y)&gt;threshold ,dst(x,y) = max_value; 否则,dst（x,y）=0;type=CV_THRESH_BINARY_INV //如果 src(x,y)&gt;threshold,dst(x,y) = 0; 否则,dst(x,y) = max_value.type=CV_THRESH_TRUNC //如果 src(x,y)&gt;threshold，dst(x,y) = max_value; 否则dst(x,y) = src(x,y).type=CV_THRESH_TOZERO //如果src(x,y)&gt;threshold，dst(x,y) = src(x,y) ; 否则 dst(x,y) = 0。type=CV_THRESH_TOZERO_INV //如果 src(x,y)&gt;threshold，dst(x,y) = 0 ; 否则dst(x,y) = src(x,y)]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[离散傅里叶变换函数的补充]]></title>
    <url>%2F2019%2F03%2F23%2FOpenCV-Learning-Day-13%2F</url>
    <content type="text"><![CDATA[前言我的上一篇的博客的关于傅里叶变换的一些函数补充，理解不易，且行且珍惜～ 测试代码在上一篇。原图：测试图； 一、getOptimalDFTSize()函数返回给定向量尺寸经过DFT变换后结果的最优尺寸大小。其函数定义：1int getOptimalDFTSize(int vecsize)int vecsize: 输入向量尺寸大小(vector size)DFT变换在一个向量尺寸上不是一个单调函数，当计算两个数组卷积或对一个数组进行光学分析，它常常会用0扩充一些数组来得到稍微大点的数组以达到比原来数组计算更快的目的。一个尺寸是2阶指数（2,4,8,16,32……）的数组计算速度最快，一个数组尺寸是2、3、5的倍数（例如：300 = 5*5*3*2*2）同样有很高的处理效率。getOptimalDFTSize()函数返回大于或等于vecsize的最小数值N，这样尺寸为N的向量进行DFT变换能得到更高的处理效率。在当前N通过p，q，r等一些整数得出N = 2^p*3^q*5^r.这个函数不能直接用于DCT（离散余弦变换）最优尺寸的估计，可以通过getOptimalDFTSize((vecsize+1)/2)*2得到。 二、magnitude()函数函数原型：1void magnitude(InputArray x, InputArray y, OutputArray magnitude)InputArray x: 浮点型数组的x坐标矢量，也就是实部InputArray y: 浮点型数组的y坐标矢量，必须和x尺寸相同OutputArray magnitude: 与x类型和尺寸相同的输出数组 三、copyMakeBorder()扩充图像边界，其函数定义如下：12345678910void copyMakeBorder( InputArray src, OutputArray dst, int top, int bottom, int left, int right, int borderType, const Scalar&amp; value=Scalar())InputArray src: 输入图像OutputArray dst: 输出图像，与src图像有相同的类型，其尺寸应为Size(src.cols+left+right, src.rows+top+bottom)int类型的top、bottom、left、right: 在图像的四个方向上扩充像素的值int borderType: 边界类型，由borderInterpolate()来定义，常见的取值为BORDER_CONSTANTconst Scalar&amp; value = Scalar(): 如果边界类型为BORDER_CONSTANT则表示为边界值 四、normalize() 函数归一化就是把要处理的数据经过某种算法的处理限制在所需要的范围内。首先归一化是为了后面数据处理的方便，其次归一化能够保证程序运行时收敛加快。归一化的具体作用是归纳同意样本的统计分布性，归一化在0-1之间是统计的概率分布，归一化在某个区间上是统计的坐标分布，在机器学习算法的数据预处理阶段，归一化也是非常重要的步骤。123456789void normalize( InputArray src, OutputArray dst, double alpha=1, double beta=0, int norm_type=NORM_L2, int dtype=-1, InputArray mask=noArray())InputArray src: 输入图像OutputArray dst: 输出图像，尺寸大小和src相同double alpha = 1: range normalization模式的最小值double beta = 0: range normalization模式的最大值，不用于norm normalization(范数归一化)模式int norm_type = NORM_L2: 归一化的类型，主要有 NORM_INF: 归一化数组的C-范数（绝对值的最大值）NORM_L1: 归一化数组的L1-范数（绝对值的和）NORM_L2: 归一化数组的L2-范数（欧几里得）NORM_MINMAX: 数组的数值被平移或缩放到一个指定的范围，线性归一化，一般较常用。int dtype = -1: 当该参数为负数时，输出数组的类型与输入数组的类型相同，否则输出数组与输入数组只是通道数相同，而depth = CV_MAT_DEPTH(dtype)InputArray mask = noArray(): 操作掩膜版，用于指示函数是否仅仅对指定的元素进行操作。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[离散傅里叶变换]]></title>
    <url>%2F2019%2F03%2F22%2FOpenCV-Learning-Day-12%2F</url>
    <content type="text"><![CDATA[前言本节学习离散傅里叶变换，好难啊～分两天学习 测试代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;string&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;int main()&#123; Mat srcIamge = imread("3.jpg",0); if(!srcIamge.data)&#123; printf("Failed!"); return 0; &#125; imshow("srcImage",srcIamge); int m = getOptimalDFTSize(srcIamge.rows); int n = getOptimalDFTSize(srcIamge.cols); Mat padded; copyMakeBorder(srcIamge,padded,0,m - srcIamge.rows,0,n - srcIamge.cols,BORDER_CONSTANT,Scalar::all(0)); Mat planes[] = &#123;Mat_&lt;float&gt;(padded),Mat::zeros(padded.size(),CV_32F)&#125;; Mat complexI; merge(planes,2,complexI); dft(complexI,complexI); split(complexI,planes); magnitude(planes[0],planes[1],planes[0]); Mat magnitudeImage = planes[0]; magnitudeImage = magnitudeImage(Rect(0,0,magnitudeImage.cols &amp; -2,magnitudeImage.rows &amp; -2)); int cx = magnitudeImage.cols / 2; int cy = magnitudeImage.rows / 2; Mat q0(magnitudeImage, Rect(0,0,cx,cy)); Mat q1(magnitudeImage, Rect(cx,0,cx,cy)); Mat q2(magnitudeImage, Rect(0,cy,cx,cy)); Mat q3(magnitudeImage, Rect(cx,cy,cx,cy)); Mat tmp; q0.copyTo(tmp); q3.copyTo(q0); tmp.copyTo(q3); q1.copyTo(tmp); q2.copyTo(q1); tmp.copyTo(q2); normalize(magnitudeImage,magnitudeImage,0,1,NORM_MINMAX); imshow("dstImage",magnitudeImage); waitKey(); return 0;&#125; 一：dft()函数参数解释：第一个参数为输入图像，可以是实数或虚数第二个参数为输出图像，其大小和类型取决于第三个参数flags第三个参数为转换的标识符，有默认值0.其可取的值如下所示：DFT_INVERSE: 用一维或二维逆变换取代默认的正向变换DFT_SCALE: 缩放比例标识符，根据数据元素个数平均求出其缩放结果，如有N个元素，则输出结果以1/N缩放输出，常与DFT_INVERSE搭配使用。DFT_ROWS: 对输入矩阵的每行进行正向或反向的傅里叶变换；此标识符可在处理多种适量的的时候用于减小资源的开销，这些处理常常是三维或高维变换等复杂操作。DFT_COMPLEX_OUTPUT: 对一维或二维的实数数组进行正向变换，这样的结果虽然是复数阵列，但拥有复数的共轭对称性（CCS），可以以一个和原数组尺寸大小相同的实数数组进行填充，这是最快的选择也是函数默认的方法。你可能想要得到一个全尺寸的复数数组（像简单光谱分析等等），通过设置标志位可以使函数生成一个全尺寸的复数输出数组。DFT_REAL_OUTPUT: 对一维二维复数数组进行逆向变换，这样的结果通常是一个尺寸相同的复数矩阵，但是如果输入矩阵有复数的共轭对称性（比如是一个带有DFT_COMPLEX_OUTPUT标识符的正变换结果），便会输出实数矩阵。int nonzeroRows = 0: 当这个参数不为0，函数会假设只有输入数组（没有设置DFT_INVERSE）的第一行或第一个输出数组（设置了DFT_INVERSE）包含非零值。这样的话函数就可以对其他的行进行更高效的处理节省一些时间，这项技术尤其是在采用DFT计算矩阵卷积时非常有效。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[亮度和对比度调节]]></title>
    <url>%2F2019%2F03%2F21%2FOpenCV-Learning-Day-11%2F</url>
    <content type="text"><![CDATA[前言本文主要介 测试代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;string&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;static void on_ContrastAndBright(int, void*);static void ShowHelpText();int g_nContrastValue;int g_nBrightValue;Mat g_srcImage, g_dstImage;int main()&#123; g_srcImage = imread("1.jpg"); if(!g_srcImage.data)&#123; printf("Failed\n"); return -1; &#125; g_dstImage = Mat::zeros(g_srcImage.size(), g_srcImage.type()); g_nContrastValue = 80; g_nBrightValue = 80; namedWindow("Output",WINDOW_AUTOSIZE); createTrackbar("Contrast:","Output",&amp;g_nContrastValue,300,on_ContrastAndBright); createTrackbar("Bright:","Output",&amp;g_nBrightValue,200,on_ContrastAndBright); on_ContrastAndBright(g_nContrastValue,0); on_ContrastAndBright(g_nBrightValue,0); while(char(waitKey(1)) != 'q') &#123;&#125; return 0; &#125;static void on_ContrastAndBright(int, void*)&#123; namedWindow("Input",WINDOW_AUTOSIZE); for(int y = 0; y &lt; g_srcImage.rows; y++) &#123; for(int x = 0; x &lt; g_srcImage.cols; x++)&#123; for(int c = 0; c &lt; 3; c++)&#123; g_dstImage.at&lt;Vec3b&gt;(y,x)[c] = saturate_cast&lt;uchar&gt; ((g_nContrastValue*0.01) * (g_srcImage.at&lt;Vec3b&gt;(y,x)[c]) + g_nBrightValue); &#125; &#125; &#125; imshow("Input",g_srcImage); imshow("Output",g_dstImage);&#125;原图：测试图；感觉图片上的时间硬是调早了两个小时…… 1.Mat::zeros()和Mat::ones()Mat::zeros()相当于创建了一张全黑的图，图像矩阵上每个像素点的每个通道全设置为0。Mat::ones()则是将图像矩阵上每个像素点的第一个通道设置为1，其余通道设置为0。 2.saturate_cast防止数据溢出原理如下：12if(data&lt;0) data=0;else if(data&gt;255) data=255; 3.Mat类中的at函数在测试代码中，函数实现亮度和对比度的调节是通过Mat类中的at函数遍历各个像素点并修改来实现的，但这不是最有效率的做法，在测试图像素为4000✖1900左右，在调节亮度和对比度时就会变得非常卡了。还有一种指针的做法和一种迭代器的做法，可以参考我之前的博客。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[图像通道的分离与合并]]></title>
    <url>%2F2019%2F03%2F20%2FOpenCV-Learning-Day-10%2F</url>
    <content type="text"><![CDATA[前言不要问，就是水，前言说没就会没。 一、通道的分离（split()函数）和合并（merge()函数）测试代码：1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;string&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;int main()&#123; Mat srcImage = imread("2.jpg"); Mat ImageBlue, ImageGreen, ImageRed; Mat mergeImage; vector&lt;Mat&gt;channels; split(srcImage,channels); ImageBlue = channels.at(0); imshow("Blue Channel",ImageBlue); ImageGreen = channels.at(1); imshow("Green Channel",ImageGreen); ImageRed = channels.at(2); imshow("Red Channel",ImageRed); merge(channels,mergeImage); imshow("mergeImage",mergeImage); waitKey(0);&#125;效果图：蓝色通道：绿色通道：红色通道：合并通道： 1.split()函数：图像通道的分离函数原型：1void split(InputArray m, OutputArrayOfArrays mv)第一个参数为输入图像第二个参数是函数的输出数组或者vector容器 2.merge()函数：图像通道的合并函数原型：1void merge(InputArrayOfArrays mv, OutputArray dst)可以看出merge的两个参数和split函数正好相反：第一个参数是保存函数通道的数组或者vector容器第二个参数为输出图像]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[通过访问图像像素减少图像颜色数量]]></title>
    <url>%2F2019%2F03%2F19%2FOpenCV-Learning-Day-9%2F</url>
    <content type="text"><![CDATA[前言由于你可以自己看下面的内容，所以没有前言。 该测试代码实现的功能是减少图像种颜色的数量，将256×256×256转换成26×26×26从而提高程序运行速度。实现该功能需要用到访问图像像素的函数和计时的函数。效果图：测试代码： 一、main函数：1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;string&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;void colorReduce(Mat&amp; inputImage,Mat&amp; outputImage,int div);int main()&#123; Mat srcImage = imread("1.jpg"); imshow("srcImage",srcImage); Mat dstImage; dstImage.create(srcImage.rows,srcImage.cols,srcImage.type()); double time0 = static_cast&lt;double&gt;(getTickCount()); colorReduce(srcImage,dstImage,32); time0 = ((double)getTickCount() - time0) / getTickFrequency(); cout &lt;&lt; "Time: " &lt;&lt; time0 &lt;&lt; "s" &lt;&lt; endl; imshow("dstImage",dstImage); waitKey(0);&#125; 1.计时函数getTickCount()函数返回CPU自某个事件以来走过的时钟周期数getTickFrequency()函数返回CPU一秒钟走过的时钟周期数两者组合如下：123double time0 = static_cast&lt;double&gt;(getTickCount());time0 = ((double)getTickCount() - time0) / getTickFrequency();cout &lt;&lt; "Time: " &lt;&lt; time0 &lt;&lt; "s" &lt;&lt; endl;即可实现对colorReduce()函数的计时。 2.static_cast函数强制类型转换，即在程序员知情的情况下进行转换，系统不会报错。 二、实现colorReduce()函数的三种方法1.指针访问函数代码：1234567891011121314void colorReduce(Mat&amp; inputImage,Mat&amp; outputImage,int div)&#123; outputImage = inputImage.clone(); int rowNumber = outputImage.rows; int colNumber = outputImage.cols * outputImage.channels(); for(int i = 0; i &lt; rowNumber; i++) &#123; uchar* data = outputImage.ptr&lt;uchar&gt;(i); for(int j = 0; j &lt; colNumber; j++) &#123; data[j] = data[j] / div * div + div / 2; &#125; &#125;&#125;Mat类中的ptr函数可以返回第i行的首地址，如：1uchar* data = outputImage.ptr&lt;uchar&gt;(i); 2.迭代器iterator函数代码：123456789101112void colorReduce(Mat&amp; inputImage,Mat&amp; outputImage,int div)&#123; outputImage = inputImage.clone(); Mat_&lt;Vec3b&gt;::iterator it = outputImage.begin&lt;Vec3b&gt;(); Mat_&lt;Vec3b&gt;::iterator itend = outputImage.end&lt;Vec3b&gt;(); for(;it != itend; ++it) &#123; (*it)[0] = (*it)[0] / div * div + div / 2; (*it)[1] = (*it)[1] / div * div + div / 2; (*it)[2] = (*it)[2] / div * div + div / 2; &#125;&#125;STL 迭代器嘻嘻嘻…自己找 3.动态地址计算函数代码：123456789101112131415void colorReduce(Mat&amp; inputImage,Mat&amp; outputImage,int div)&#123; outputImage = inputImage.clone(); int rowNumber = outputImage.rows; int colNumber = outputImage.cols; for(int i = 0; i &lt; rowNumber; i++) &#123; for(int j = 0; j &lt; colNumber; j++) &#123; outputImage.at&lt;Vec3b&gt;(i,j)[0] = outputImage.at&lt;Vec3b&gt;(i,j)[0] / div * div + div / 2; outputImage.at&lt;Vec3b&gt;(i,j)[1] = outputImage.at&lt;Vec3b&gt;(i,j)[1] / div * div + div / 2; outputImage.at&lt;Vec3b&gt;(i,j)[2] = outputImage.at&lt;Vec3b&gt;(i,j)[2] / div * div + div / 2; &#125; &#125;&#125;Mat类中的at函数可以读取和修改图像矩阵中对应坐标的元素，但是必须在编译时知道图像的数据类型，at函数本身不会进行任何数据类型的转换，因此需要确保我们指定的数据类型要和图像矩阵中的数据类型相同。一般形式：1image.at&lt;Vec3b&gt;(i,j)[channel] = value;在本程序给出的图像是三通道的，因此是Vec3b，表示三个八位数的向量。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[简单图形绘制]]></title>
    <url>%2F2019%2F03%2F18%2FOpenCV-Learning-Day-8%2F</url>
    <content type="text"><![CDATA[前言看毛星云大大的教程的测试代码就把我看晕了……所以没有前言。 测试代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;string&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;using namespace std;using namespace cv;#define WINDOW_WIDTH 600#define WINDOW_NAME1 "Picture1"#define WINDOW_NAME2 "Picture2"void DrawEllipse(Mat img, double angle);void DrawFilledCircle(Mat img,Point center);void DrawLine(Mat img,Point start,Point end);void DrawPolygon(Mat img);int main()&#123; Mat atomImage = Mat::zeros(WINDOW_WIDTH,WINDOW_WIDTH,CV_8UC3); Mat rookImage = Mat::zeros(WINDOW_WIDTH,WINDOW_WIDTH,CV_8UC3); DrawEllipse(atomImage,90); DrawEllipse(atomImage,0); DrawEllipse(atomImage,45); DrawEllipse(atomImage,-45); DrawFilledCircle(atomImage,Point(WINDOW_WIDTH/2,WINDOW_WIDTH/2)); DrawPolygon(rookImage); rectangle(rookImage, Point(0,7*WINDOW_WIDTH/8), Point(WINDOW_WIDTH,WINDOW_WIDTH), Scalar(0,255,255), -1, 8); 9) DrawLine(rookImage,Point(0,15*WINDOW_WIDTH/16),Point(WINDOW_WIDTH,15*WINDOW_WIDTH/16)); DrawLine(rookImage,Point(WINDOW_WIDTH/4,7*WINDOW_WIDTH/8),Point(WINDOW_WIDTH/4,WINDOW_WIDTH)); DrawLine(rookImage,Point(WINDOW_WIDTH/2,7*WINDOW_WIDTH/8),Point(WINDOW_WIDTH/2,WINDOW_WIDTH)); DrawLine(rookImage,Point(3*WINDOW_WIDTH/4,7*WINDOW_WIDTH/8),Point(3*WINDOW_WIDTH/4,WINDOW_WIDTH)); imshow(WINDOW_NAME1,atomImage); moveWindow(WINDOW_NAME1,0,200); imshow(WINDOW_NAME2,rookImage); moveWindow(WINDOW_NAME2,WINDOW_WIDTH,200); waitKey(0); return(0);&#125;void DrawEllipse(Mat img, double angle)&#123; int thickness = 2; int linetype = 8; ellipse(img, Point(WINDOW_WIDTH/2,WINDOW_WIDTH/2), Size(WINDOW_WIDTH/4,WINDOW_WIDTH/16), angle, 0, 360, Scalar(255,129,0), thickness, linetype);&#125;void DrawFilledCircle(Mat img,Point center)&#123; int thickness = -1; int linetype = 8; circle(img, center, WINDOW_WIDTH/32, Scalar(0,0,255), thickness, linetype);&#125;void DrawPolygon(Mat img)&#123; int linetype = 8; Point rookPoints[1][20]; rookPoints[0][0] = Point(WINDOW_WIDTH/4,7*WINDOW_WIDTH/8); rookPoints[0][1] = Point(3*WINDOW_WIDTH/4,7*WINDOW_WIDTH/8); rookPoints[0][2] = Point(3*WINDOW_WIDTH/4,13*WINDOW_WIDTH/16); rookPoints[0][3] = Point(11*WINDOW_WIDTH/16,13*WINDOW_WIDTH/16); rookPoints[0][4] = Point(19*WINDOW_WIDTH/32,3*WINDOW_WIDTH/8); rookPoints[0][5] = Point(3*WINDOW_WIDTH/4,3*WINDOW_WIDTH/8); rookPoints[0][6] = Point(3*WINDOW_WIDTH/4,WINDOW_WIDTH/8); rookPoints[0][7] = Point(26*WINDOW_WIDTH/40,WINDOW_WIDTH/8); rookPoints[0][8] = Point(26*WINDOW_WIDTH/40,WINDOW_WIDTH/4); rookPoints[0][9] = Point(22*WINDOW_WIDTH/40,WINDOW_WIDTH/4); rookPoints[0][10] = Point(22*WINDOW_WIDTH/40,WINDOW_WIDTH/8); rookPoints[0][11] = Point(18*WINDOW_WIDTH/40,WINDOW_WIDTH/8); rookPoints[0][12] = Point(18*WINDOW_WIDTH/40,WINDOW_WIDTH/4); rookPoints[0][13] = Point(14*WINDOW_WIDTH/40,WINDOW_WIDTH/4); rookPoints[0][14] = Point(14*WINDOW_WIDTH/40,WINDOW_WIDTH/8); rookPoints[0][15] = Point(WINDOW_WIDTH/4,WINDOW_WIDTH/8); rookPoints[0][16] = Point(WINDOW_WIDTH/4,3*WINDOW_WIDTH/8); rookPoints[0][17] = Point(13*WINDOW_WIDTH/32,3*WINDOW_WIDTH/8); rookPoints[0][18] = Point(5*WINDOW_WIDTH/16,13*WINDOW_WIDTH/16); rookPoints[0][19] = Point(WINDOW_WIDTH/4,13*WINDOW_WIDTH/16); const Point* ppt[1] = &#123;rookPoints[0]&#125;; int npt[] = &#123;20&#125;; fillPoly(img, ppt, npt, 1, Scalar(255,255,255), linetype);&#125;void DrawLine(Mat img,Point start,Point end)&#123; int thinkness = 2; int linetype = 8; line(img, start, end, Scalar(0,0,0), thinkness, linetype);&#125;效果图： 一、void DrawEllipse(Mat img, double angle)该函数调用了OpenCV中的ellipse函数用以绘制椭圆。函数原型：1234567891011121314151617181920void ellipse( Mat&amp; img, Point center, Size axes, double angle, double startAngle, double endAngle, const Scalar&amp; color, int thickness=1, int lineType=8, int shift=0)void ellipse( Mat&amp; img, const RotatedRect&amp; box, const Scalar&amp; color, int thickness=1, int lineType=8) 二、void DrawFilledCircle(Mat img,Point center)该函数调用circle函数用以绘制圆形。函数原型：123456789void circle( InputOutputArray img, Point center, int radius, const Scalar &amp;color, int thickness = 1, int lineType = 8, int shift = 0)在测试程序中，由于线粗设置为-1，因此圆形是实心的。 三、void DrawPolygon(Mat img)该函数调用了fillPoly函数用以绘制自定义的多边形。函数原型：12345678910void fillPoly( InputOutputArray img, const Point **pts, const int *npts, int ncontours, const Scalar &amp;color, int lineType = 8, int shift = 0, Point offset = Point())在测试程序中，该函数的多边形的顶点集为ppt，需绘制的多边形顶点数目为npt，绘制图形数量为1。 四、void DrawLine(Mat img,Point start,Point end)函数原型：123456789void line( InputOutputArray img, Point pt1, Point pt2, const Scalar &amp;color, int thickness = 1, int lineType = 8, int shift = 0)无甚可讲。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[鼠标操作初级]]></title>
    <url>%2F2019%2F03%2F17%2FOpenCV-Learning-Day-7%2F</url>
    <content type="text"><![CDATA[前言因为今天天气不错，所以没有前言。 测试代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;#define WINDOW_NAME "SetMouseCallback Sample"void on_MouseHandle(int event,int x,int y,int flags,void* param);void DrawRectangle(Mat&amp; img,Rect box);void ShowHelpText();Rect g_rectangle;bool g_bDrawingBox = false;RNG g_rng(12345);int main(int argc,char** argv)&#123; g_rectangle = Rect(-1,-1,0,0); Mat srcImage(600,800,CV_8UC3),tempImage; srcImage.copyTo(tempImage); srcImage = Scalar::all(0); namedWindow(WINDOW_NAME); setMouseCallback(WINDOW_NAME,on_MouseHandle,(void*)&amp;srcImage); while(1)&#123; srcImage.copyTo(tempImage); if(g_bDrawingBox) DrawRectangle(tempImage, g_rectangle); imshow(WINDOW_NAME,tempImage); if(waitKey(10) == 27) break; &#125; return 0;&#125;void on_MouseHandle(int event,int x,int y,int flags,void* param)&#123; Mat&amp; image = *(Mat*) param; switch (event) &#123; //鼠标移动消息 case EVENT_MOUSEMOVE: &#123; if (g_bDrawingBox) &#123; g_rectangle.width = x - g_rectangle.x; g_rectangle.height = y - g_rectangle.y; &#125; &#125; break; //左键按下消息 case EVENT_LBUTTONDOWN: &#123; g_bDrawingBox = true; g_rectangle = Rect(x,y,0,0); &#125; break; //左键抬起消息 case EVENT_LBUTTONUP: &#123; g_bDrawingBox = false; //对宽和高小于0的处理 if (g_rectangle.width &lt; 0) &#123; g_rectangle.x += g_rectangle.width; g_rectangle.width *= -1; &#125; DrawRectangle(image,g_rectangle); &#125; break; &#125;&#125;void DrawRectangle(Mat&amp; img,Rect box)&#123; rectangle(img,box.tl(),box.br(),Scalar(g_rng.uniform(0,255),g_rng.uniform(0,255),g_rng.uniform(0,255)));&#125;效果图：setMouseCallback()函数函数原型：12345void setMouseCallback( const String &amp;winname, MouseCallback onMouse, void *userdata = (void *)0)第一个参数为窗口名称第二个参数为MouseCallback类型的onMouse，窗口里每次鼠标操作的发生都会调用这个函数，这个函数的原型：1234567void on_MouseHandle( int event, int x, int y, int flags, void *param)event是许多EVENT_变量，这个在测试程序中有所体现；x和y是在图像坐标系（？）的坐标值，flags是EVENT_FLAG的组合，param是用户定义的传递到setMouseCallback()函数调用的参数。第三个参数是用户定义的传递到回调函数的参数，有默认值0 一、Scalar()函数：函数定义：1234typedef struct Scalar&#123; double val[4];&#125;Scalar;由函数定义可知Scalar()可以存储4个double类型的值，分别对应4通道（需要注意的是，在OpenCV中，前三个颜色通道分别是BGR而不是常识中的RGB，第四个通道则是透明度Alpha值），在使用Scalar()函数的过程中，具体需要使用几个值需要看Mat类型图像的type，比如常见的CV_8UC3类型，C就代表通道（channel），看到C3就知道这个图像有三个通道，因此应该写三个值，每个值分别赋值给对应通道内的所有矩阵元素，如果出现参数比通道数少的情况，那么未赋值的通道内所有矩阵元素全部为0。另科普Mat类型的type参数：1.bit_depth：比特数，代表8bite,16bites,32bites,64bites例如创建一个存储灰度图片的Mat对象,这个图像的大小为宽100,高100,那么,现在这张灰度图片中有10000个像素点，它每一个像素点在内存空间所占的空间大小是8bite,8位，所以它对应的就是CV_82.S|U|FS代表signed int有符号整形U代表unsigned int无符号整形F代表float单精度浮点型3.C（channel）代表一张图片的通道数灰度图片grayImg是单通道图像RGB彩色图像是3通道图像带Alph通道的RGB图像是4通道图像 二、Rect()函数函数定义：1234567typedef struct Rect &#123; int x; /* 方形的左上角的x-坐标 */ int y; /* 方形的左上角的y-坐标*/ int width; /* 宽 */ int height; /* 高 */ &#125;函数用法：123456789101112131415161718192021222324252627282930313233343536373839//如果创建一个Rect对象rect(100, 50, 50, 100)，那么rect会有以下几个功能： rect.area(); //返回rect的面积 5000 rect.size(); //返回rect的尺寸 [50 × 100] rect.tl(); //返回rect的左上顶点的坐标 [100, 50] rect.br(); //返回rect的右下顶点的坐标 [150, 150] rect.width(); //返回rect的宽度 50 rect.height(); //返回rect的高度 100 rect.contains(Point(x, y)); //返回布尔变量，判断rect是否包含Point(x, y)点 //还可以求两个矩形的交集和并集 rect = rect1 &amp; rect2; rect = rect1 | rect2; //还可以对矩形进行平移和缩放 rect = rect + Point(-100, 100); //平移，也就是左上顶点的x坐标-100，y坐标+100 rect = rect + Size(-100, 100); //缩放，左上顶点不变，宽度-100，高度+100 //还可以对矩形进行对比，返回布尔变量 rect1 == rect2; rect1 != rect2; //判断rect1是否在rect2里面bool isInside(Rect rect1, Rect rect2) &#123; return (rect1 == (rect1&amp;rect2)); &#125; //矩形中心点Point getCenterPoint(Rect rect) &#123; Point cpt; cpt.x = rect.x + cvRound(rect.width/2.0); cpt.y = rect.y + cvRound(rect.height/2.0); return cpt; &#125; //围绕矩形中心缩放 Rect rectCenterScale(Rect rect, Size size) &#123; rect = rect + size; Point pt; pt.x = cvRound(size.width/2.0); pt.y = cvRound(size.height/2.0); return (rect-pt); &#125; Rect()函数用法摘自：【OpenCV】cv::Rect矩形类用法blog.csdn.net/qq_30214939/article/details/65648273 三、Rectangle()函数函数原型：123456789void rectangle( InputOutputArray img, Point pt1, Point pt2, const Scalar&amp; color, int thickness = 1, int lineType = LINE_8, int shift = 0)第一个参数是要处理的图片第二和第三个参数分别是矩形的左上角和右下角的坐标第四个参数是矩形的颜色第五个参数是线的粗细第六个参数是线形 四、RNG随机数类型RNG可以产生3种随机数：RNG(int seed)使用种子seed产生一个64位随机整数，默认-1（计算机的伪随机数是由随机种子根据一定的计算方法计算出来的数值，所以只要计算方法一定，随机种子一定，那么产生的随机数就是固定的）RNG::uniform()产生一个均匀分布的随机数（RNG::uniform(a, b )返回一个[a,b)范围的均匀分布的随机数，a,b的数据类型要一致，而且必须是int、float、double中的一种，默认是int）RNG::gaussian( )产生一个高斯分布的随机数（返回一个均值为0，标准差为σ的随机数。如果要产生均值为λ，标准差为σ的随机数，可以λ+ RNG::gaussian( σ)） 五、Mat&amp; image = (Mat) param;的解释param是用户定义的传递到setMouseCallback()函数调用的参数,在本行代码中先将param强制转换为Mat类型指针，然后取param的值赋值给左边作为Mat类型引用的image变量。]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[滑动条创建和使用]]></title>
    <url>%2F2019%2F03%2F16%2FOpenCV-Learning-Day-6%2F</url>
    <content type="text"><![CDATA[前言没有前言。 测试代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;const int g_nMaxAlphaValue = 100; //Alpha值最大值int g_nAlphaValueSlider;//滑动条对应变量double g_dAlphaValue;double g_dBetaValue;Mat g_srcImage1;Mat g_srcImage2;Mat g_dstImage;void on_Trackbar(int, void*)&#123; //Alpha和Beta值的比例 g_dAlphaValue = (double) g_nAlphaValueSlider / g_nMaxAlphaValue; g_dBetaValue = (1.0 - g_dAlphaValue); //根据Alpha和Beta线性混合 addWeighted(g_srcImage1,g_dAlphaValue,g_srcImage2,g_dBetaValue,0,g_dstImage); imshow("Trackbar Sample",g_dstImage);&#125;int main(int argc,char** argv)&#123; //图像尺寸必须相同！！！ g_srcImage1 = imread("Paint1.jpg"); g_srcImage2 = imread("Paint2.jpg"); if(!g_srcImage1.data) &#123; printf("Reading image 1 failed\n"); return -1; &#125; if(!g_srcImage2.data) &#123; printf("Reading image 2 failed\n"); return -1; &#125; g_nAlphaValueSlider = 70; namedWindow("Trackbar Sample",1); char TrakbarName[50]; sprintf(TrakbarName,"Alpha"); createTrackbar(TrakbarName,"Trackbar Sample",&amp;g_nAlphaValueSlider,g_nMaxAlphaValue,on_Trackbar); //运行时初始化界面 on_Trackbar(g_nAlphaValueSlider,0); waitKey(0); return 0;&#125;效果图；Alpha为0时：Alpha为100时： 1.createTrackbar() 函数函数原型：12345678int createTrackbar( const string&amp; trackbarname, const string&amp; winname, int* value, int count, TrackbarCallback onChange=0, void* userdata=0)第一个参数：轨迹条的名字第二个参数：窗口的名字第三个参数：int*类型的指针，指向滑块位置，创建时滑块的位置就是该变量当前的值第四个参数：滑块能达到的最大位置的值第五个参数：TrackbarCallback回调函数，默认为零，滑动条的每一次变化都会调用这个函数，函数原型必须是void XXX(int, void*)，第一个参数是轨迹条的位置，第二个参数是用户传给回调函数的参数，如果第三个参数是全局变量的话，完全可以不用管第六个参数。需要注意的是，读取的图像尺寸必须相同，不然会报错。2.getTrackbarPos()：获取滑动条的位置的值函数原型：1234int getTrackbarPos( const string&amp; trackbarname, const string&amp; winname)]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[图像的简单载入、显示和输出]]></title>
    <url>%2F2019%2F03%2F15%2FOpenCV-Learning-Day-5%2F</url>
    <content type="text"><![CDATA[前言本章主要介绍图像的简单载入、显示和输出，新内容有namedWindow()函数、图像的ROI（感兴趣区域）以及图像的输出imwrite()函数，顺带科普一下c++文件中常常定义在main函数中的两个形参argc和argv究竟是什么意思～ 一、图像的载入、显示和输出测试代码：123456789101112131415161718192021222324252627282930#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;int main()&#123; Mat image = imread("Girl.jpg"); Mat logo = imread("logo.jpg"); namedWindow("Girl"); imshow("Girl",image); namedWindow("logo"); imshow("logo",logo); Mat imageROI; imageROI = image(Rect(1600,2100,logo.cols,logo.rows)); //imageROI = image(Range(2100,2100+logo.rows),Range(1600,1600+logo.cols)); addWeighted(imageROI,0.5,logo,1,0,imageROI); namedWindow("Girl+logo"); imshow("Girl+logo",image); imwrite("Output.jpg",image); waitKey(0); return 0;&#125; 原图： 效果图； 1.为什么会有namedWindow()函数？在简单的图像处理中，我们通常只要读取图像，经过处理后使用imshow()函数输出图像即可，但是在进阶操作中，我们可能会在执行代码生成的窗口上放置滚动条或者按钮，这时候我们就需要在代码执行前就定义这个窗口，这就是namedWindow()函数的作用。函数原型：1234void nameWindow( const string&amp; winname, int flags = WINDOW_AUTOSIZE)其中第二个参数可以设置窗口能否放缩以及是否支持OpenGL。2.ROI（region of interest）感兴趣区域圈定一块图像中需要处理的区域，既节省性能，又便于操作。需要定义一个Mat类型以存放图像的ROI，设置ROI其实就是在原来图片上指定一个区域，而这个区域只是新创建了一个图片文件的头信息而已并没有产生新的图片，文件头里的图片区域的起始位置指向了ROI区域的左上角位置，所以在ROI上做的任何操作都会影响原图片。设置ROI区域有两种方法，测试代码里的注释是另一种，两者效果相同。3.imwrite()函数：输出图像到文件。函数原型：12345bool imwrite( const string&amp; filename, InputArray img, const vector&lt;int&gt;&amp; params=vector&lt;int&gt;())注意：第一个参数文件名要带上后缀 二、argc和argvargc、argv中的arg指的是参数（argument），因此argc的全称为argument counter和argument vector，其中argc为整数，用来统计运行程序时送给main函数的命令行参数的个数；而*argv[]:为字符串数组用来存放指向的字符串参数的指针数组，每一个元素指向一个参数。各成员含义如下：argv[0]指向程序运行的全路径名argv[1]指向在命令行中执行程序名后的第一个字符串argv[2]指向在命令行中执行程序名后的第二个字符串argv[3]指向在命令行中执行程序名后的第三个字符串argv[argv]为NULL]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基本视频读取和操作]]></title>
    <url>%2F2019%2F03%2F14%2FOpenCV-Learning-Day-4%2F</url>
    <content type="text"><![CDATA[前言本文主要介绍VideoCapture()函数的使用，包括读取视频和调用摄像头等操作，并结合之前学的一些基本图像处理操作实现视频中物体边缘显示并给出测试代码。对于Mac OS X 10.14无法调用摄像头的问题，文中将给出解决方法。 一、读取视频测试代码：1234567891011121314151617#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;int main()&#123; VideoCapture capture("Adventure.mp4"); //循环显示每一帧 while(1)&#123; Mat frame; capture &gt;&gt; frame; imshow("Video",frame); waitKey(30); &#125; return 0;&#125; 对于声明为VideoCapture类型的参数，只要将其初始化为视频文件的相对路径或者绝对路径，就可以读取视频文件。定义一个Mat类型的变量，通过while循环将视频的每一帧读取到Mat变量中，这样就可以实现图像操作。 二、调用摄像头测试代码：1234567891011121314151617181920212223242526#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;string&gt;using namespace cv;using namespace std;int main()&#123; VideoCapture capture(0); //循环显示每一帧 while(1)&#123; Mat srcImage; capture &gt;&gt; srcImage; Mat grayImage, dstImage; Mat grad_x,abs_grad_x,grad_y,abs_grad_y; cvtColor(srcImage,grayImage,COLOR_BGR2GRAY); blur(grayImage,dstImage,Size(3,3)); Sobel(grayImage,grad_x,CV_16S,1,0,3); convertScaleAbs(grad_x,abs_grad_x); Sobel(grayImage,grad_y,CV_16S,0,1,3); convertScaleAbs(grad_y,abs_grad_y); addWeighted(abs_grad_x,0.5,abs_grad_y,0.5,0,dstImage); imshow("dstImage",dstImage); waitKey(30); &#125; return 0;&#125; 对于声明为VideoCapture类型的参数，只要将其初始化为0，就表示调用摄像头。定义一个Mat类型的变量，通过while循环将视频的每一帧读取到Mat变量中，这样就可以实现图像操作。测试代码实现的功能是调用摄像头后通过sobel算子勾勒出物体轮廓。具体sobel函数实现可参考我的上一篇博客：三种基本边缘检测算子www.whoiscaesarbao.com/2019/03/13/OpenCV-Learning-Day-3需要注意的是，Mac OS X 10.14版本存在摄像头无法调用的问题，原因是摄像头权限没给，因此我们要在应用程序文件夹里找到macOS自带的Photo Booth.app，右键选择显示包内容，文件夹里有一个Info.plist文件，默认使用Xcode打开并删去不必要的项，并加入最后一项Privacy - Camera Usage Description 设置为YES，配置后如下图：其中OpenCV是我的项目名，将编辑好的Info.plist文件拷贝到：Xcode为与项目文件夹并列VS Code为与生成的可执行文件并列这样就有权限调用摄像头了。 今天有点水，做数学作业去了～]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[三种基本边缘检测算子]]></title>
    <url>%2F2019%2F03%2F13%2FOpenCV-Learning-Day-3%2F</url>
    <content type="text"><![CDATA[前言本片主要介绍三个基本边缘检测的算子（canny算子、sobel算子、laplace算子），关于三个算子的原理和介绍ck2016的简书博客：数字图像 - 边缘检测原理 - Sobel, Laplace, Canny算子www.jianshu.com/p/2334bee37de5 介绍的非常通俗易懂和详细，下面主要放函数原型，测试代码和图片展示～ 原图： 一、canny算子：函数原型：123456789101112void cv::Canny ( InputArray image, //输入图像，必须为单通道灰度图 OutputArray edges, //输出图像，为单通道黑白图 double threshold1, double threshold2, //第三个参数和第四个参数表示阈值，这二个阈值中当中的小阈值用来控制边缘连接，大的阈值用来控制强边缘的初始分割，即如果一个像素的梯度大于上限值，则被认为是边缘像素，如果小于下限阈值，则被抛弃。如果该点的梯度在两者之间则当这个点与高于上限值的像素点连接时我们才保留，否则删除。 int apertureSize = 3, //第五个参数表示Sobel 算子大小，默认为3即表示一个3*3的矩阵。Sobel 算子与高斯拉普拉斯算子都是常用的边缘算子 bool L2gradient = false ) 测试代码：1234567891011121314151617181920#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123; Mat srcImage = imread("Girl.jpg"); imshow("srcImage",srcImage); Mat grayImage, dstImage; cvtColor(srcImage,grayImage,COLOR_BGR2GRAY); blur(grayImage,dstImage,Size(3,3)); Canny(dstImage,dstImage,3,9,3); imshow("dstImage",dstImage); waitKey(0);&#125; 效果图： 需要注意的是，canny算子在算出梯度值后会有勾勒边缘的一步：把区域内不是极值的点全部置0，因此效果图中的边缘会变成细线，但同时也会导致一些弱的边缘会被抹去，因此canny算子提供了两个阈值的设置，数值超过大阈值的像素点会被认为是边缘，低于小阈值的像素点被认为不是边缘，数值处于两个阈值之间的，将由周围已经被认为是边缘的像素点开始走格子，可达的是边缘，不可达的被认为不是。阈值的大小比最好为2：1或3：1。 二、sobel算子函数原型：1234567891011void Sobel( InputArray src, //输入图像 OutputArray dst, //输出图像 int ddepth,//目标图像的颜色深度 int dx, //取1表示对x求一阶导数，用来检测竖直边缘 int dy, //取1表示对y求一阶导数，用来检测水平边缘 int ksize=3, //sobel核的大小，必须是奇数，默认为3 double scale=1, double delta=0, int borderType=BORDER_DEFAULT)测试代码：123456789101112131415161718192021222324252627#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123; Mat srcImage = imread("Girl.jpg"); imshow("scrImage",srcImage); Mat grayImage, dstImage; Mat grad_x,abs_grad_x,grad_y,abs_grad_y; //取灰度图和模糊降噪 cvtColor(srcImage,grayImage,COLOR_BGR2GRAY); blur(grayImage,dstImage,Size(3,3)); Sobel(grayImage,grad_x,CV_16S,1,0,3); //sobel算子使用16位有符号的数据类型,防止截断 convertScaleAbs(grad_x,abs_grad_x); //用convertScaleAbs()函数将其转回原来的uint8形式 Sobel(grayImage,grad_y,CV_16S,0,1,3); convertScaleAbs(grad_y,abs_grad_y); addWeighted(abs_grad_x,0.5,abs_grad_y,0.5,0,dstImage); imshow("dstImage",dstImage); waitKey(0);&#125; 效果图： 1.需要注意的是，sobel算子的颜色深度最好使用CV_16S，因为OpenCV文档中对Sobel算子的介绍中有这么一句：“in the case of 8-bit input images it will result in truncated derivatives”。即Sobel函数求完导数后会有负值，还有会大于255的值。而原图像是uint8，即8位无符号数，所以Sobel建立的图像位数不够，会有截断，因此要使用16位有符号的数据类型，即cv2.CV_16S。因此在计算完之后要用convertScaleAbs()函数将其转回原来的uint8形式。2.addWeighted()函数是将两张相同大小，相同类型的图片融合的函数。 函数原型：12345678void cvAddWeighted( const CvArr* src1, //第一个原数组 double alpha,//第一个数组元素权重 const CvArr* src2, //第二个原数值 double beta,//第二个数组元素权重 double gamma, //两个数组作和后添加的数值。不要太大，不然图片一片白。总和等于255以上就是纯白色了 CvArr* dst //输出图像) 三、laplace算子函数原型：123456789void Laplacian( src_gray, //输入图像 dst, //输出图像 ddepth, //因为输入图像一般为CV_8U，为了避免数据溢出，输出图像深度应该设置为CV_16S kernel_size, //核大小，默认为3 scale, delta, BORDER_DEFAULT) 测试代码：123456789101112131415161718192021222324#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123; Mat srcImage = imread("Girl.jpg"); imshow("srcImage",srcImage); Mat grayImage, dstImage; //取灰度图和模糊降噪 cvtColor(srcImage,grayImage,COLOR_BGR2GRAY); blur(grayImage,dstImage,Size(3,3)); Laplacian(grayImage,dstImage,CV_16S,3); convertScaleAbs(dstImage,dstImage); imshow("dstImage",dstImage); waitKey(0);&#125; 效果图： 需要注意的是，由于输入图像是CV_8U格式，但是在使用laplace是为了防止截断需要转换成CV_16S格式，在计算完之后需要convertScaleAbs()函数转换回CV_8U格式，否则是一张灰色图像，别问我怎么知道的。 总结：sobel算子产生的边缘有强弱，抗噪性好。laplace算子对边缘敏感，可能有些是噪声的边缘，也被算进来了。canny算走产生的边缘很细，可能就一个像素那么细，没有强弱之分。 参考资料：【OpenCV入门教程之十二】OpenCV边缘检测：Canny算子,Sobel算子,Laplace算子,Scharr滤波器合辑blog.csdn.net/poem_qianmo/article/details/25560901 数字图像 - 边缘检测原理 - Sobel, Laplace, Canny算子www.jianshu.com/p/2334bee37de5 OpenCV-Python教程（6、Sobel算子）blog.csdn.net/sunny2038/article/details/9170013 Opencv--Sobel算子blog.csdn.net/qq_41248872/article/details/82886228 opencv中addWeighted()函数用法总结（05）blog.csdn.net/fanjiule/article/details/81607873]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[五种基本滤波器]]></title>
    <url>%2F2019%2F03%2F12%2FOpenCV-Learning-Day-2%2F</url>
    <content type="text"><![CDATA[前言本片主要介绍五个常见的滤波器（线性滤波、方框滤波、高斯滤波、中值滤波、双边滤波），文章基本是我翻阅博客整理而成的笔记～目前水平太菜了自己写不了基础知识，只能整理大神们的文章了！ 测试代码：1234567891011121314151617181920212223242526272829#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123; Mat srcImage = imread("Bilbo.jpg"); imshow("Input Image",srcImage); Mat dstImage; blur(srcImage,dstImage,Size(3,3)); imshow("blur",dstImage); boxFilter(srcImage,dstImage,-1,Size(3,3)); imshow("boxFilter",dstImage); GaussianBlur(srcImage,dstImage,Size(3,3),0,0); imshow("GaussianBlur",dstImage); medianBlur(srcImage,dstImage,3); imshow("medianBlur",dstImage); bilateralFilter(srcImage,dstImage,100,0,0); imshow("bilateralFilter",dstImage); waitKey(0);&#125; 效果图： 一、基本滤波器1.线性滤波均值滤波blur：将一个区域内的像素值求和取平均值，然后用这个平均值替换区域中心的像素值。计算速度很快，但是在去躁的同时会模糊很多细节部分，不容易保存细节。 函数原型：1234567void blur( InputArray src, //输入图像 OutputArray dst, //输出图像 Size ksize, //内核大小 Point anchor = Point(-1,-1), //锚点位置，默认为中心点 int borderType = BORDER_DEFAULT //用于推断图像外部像素的某种边界模式，默认值BORDER_DEFAULT) 效果图： 方框滤波boxFilter：算法和均值滤波相似，优缺点也相同，均值滤波算是方框滤波的特殊版本。 效果图： 高斯滤波GaussianBlur：高斯滤波是专门用于消除满足高斯分布(正态分布)的误差而存在的滤波，此时邻域算子是专门的高斯核，图像中的像素与高斯核做卷积，生成的结果加权平均存放到目标像素中，权重按照二维正态分布，对于抑制符合正态分布的噪声非常有效，并可以增强图像值不同比例下的图像效果。 函数原型：123456789void GaussianBlur( InputArray src, OutputArray dst, Size ksize, //表示高斯核函数在Y方向的的标准偏差，若sigmaY为零，就将它设为sigmaX，如果sigmaX和sigmaY都是0，那么就由ksize.width和ksize.height计算出来 double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT) 效果图： 2.非线性滤波中值滤波medianBlur：基本思想就是用像素点的领域灰度的中值来代替该像素点的灰度值，该方法在去除脉冲噪声、椒盐噪声的同时又能保留图像的细节（不会出现边缘模糊的情况）。中值滤波跟均值滤波的思想看起来很相似，只是一个取平均值，一个取中位数，但是均值滤波的计算速度是中值滤波的五倍，具体原因可以看总结。 函数原型：12345void medianBlur( InputArray src, OutputArray dst, int ksize //孔径的线性尺寸（aperture linear size），必须为奇数) 效果图： 双边滤波bilateralFilter：双边滤波的最大特点就是做边缘保存，保边去噪。大概的原理是给定一个范围，如果某些区域点与点之间如果数值差距超出范围，那么这个区域就不滤波了，能够很好的保留图像轮廓。函数原型：12345678void bilateralFilter( InputArray src, OutputArray dst, int d, //在过滤过程中每个像素邻域的直径范围 double sigmaColor, //颜色空间过滤器的sigma值 double sigmaSpace, //坐标空间中滤波器的sigma值 int borderType=BORDER_DEFAULT //用于推断图像外部像素的某种边界模式，默认值BORDER_DEFAULT) 注：对sigmaColor、和sigmaSpace两个参数存在比较大的疑惑，因为我不管怎么调后面的参数，效果图好像和原图没有差别…效果图： 3.线性滤波和非线性滤波的区别线性滤波器的原始数据与滤波结果是一种算术运算，即用加减乘除等运算实现，如均值滤波器（模板内像素灰度值的平均值）、高斯滤波器（高斯加权平均值）等。由于线性滤波器是算术运算，有固定的模板，因此滤波器的转移函数是可以确定并且是唯一的（转移函数即模板的傅里叶变换）。 非线性滤波器的原始数据与滤波结果是一种逻辑关系，即用逻辑运算实现，如最大值滤波器、最小值滤波器、中值滤波器等，是通过比较一定邻域内的灰度值大小来实现的，没有固定的模板，因而也就没有特定的转移函数（因为没有模板作傅里叶变换），另外，膨胀和腐蚀也是通过最大值、最小值滤波器实现的。（这就是为什么均值滤波要比中值滤波快五倍的原因：需要通过排序确定中值后才能生成模版） 参考资料：什么是线性滤波、非线性滤波https://www.cnblogs.com/snowxshy/p/3855011.htmlopencv中的各种滤波https://www.jianshu.com/p/8f4024742821OpenCv基本滤波算法小结 blockquotehttps://blog.csdn.net/fzhykx/article/details/79546549opencv中的各种滤波函数https://blog.csdn.net/zoucharming/article/details/70197863OpenCV探索之路（三）：滤波操作https://www.cnblogs.com/skyfsm/p/6873188.html]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[了解卷积：深度学习中的卷积之我见]]></title>
    <url>%2F2019%2F03%2F11%2FSomething-Fundamental-about-Convolution%2F</url>
    <content type="text"><![CDATA[一、前言最近在学习OpenCV的过程中接触到了几个常见的算子（canny算子、sobel算子、laplace算子），并对其中的卷积产生了非常大的困惑，此为其一；同时，卷积也可以说是深度学习当中最重要的概念，如CNN神经网络（Conventional Neural Net）更是深度学习中最为常见，也是最为优秀的神经网络之一，此为其二。可以说，卷积是一道不得不迈的门槛，迈过去能办事，迈不过去就只能吹牛了。在翻阅许多博客之后，我发现每个人对卷积的理解都不尽相同，每一篇都能让我有或多或少的收获，但没有一篇能让我完全理解，因此我想站在自己的角度上，稍微谈谈我对卷积以及卷积在深度学习中的使用的一些浅见。 二、一维卷积及其意义在数学中，卷积地表示是一维的。我们称 (f*g)(n) 为 f,g 的卷积，其连续的定义为：(f∗g)(n)=∫∞−∞f(τ)g(n−τ)dτ其离散的定义为：(f∗g)(n)=∑−∞∞f(τ)g(n−τ) 好家伙，看是看得懂，可卷积出来的结果究竟是个什么东西？它的意义是什么？我看到的最能让我明白的是这两个答案：如何通俗易懂地解释卷积？ - 张俊博的回答 - 知乎https://www.zhihu.com/question/22298352/answer/34267457卷积为什么叫「卷」积？ - 荆哲的回答 - 知乎https://www.zhihu.com/question/54677157/answer/141245297 看完你理解一下就会知道，卷积的一个重要意义就是：一个函数在另一个函数上的加权叠加。在深度学习中，我们把一个函数看作是输入的图像代表的矩阵（Input Image），把另一个图像看作是卷积核（Convolution Kernel），两者卷积，就是输入图像和卷积核的加权叠加，权重越大，卷积值越大，就代表越重要。这是一个陌生的概念，我先按下不提。我之所以会提及一维卷积，是因为一维的卷积更能帮助我们理解卷积的意义：加权叠加。这就是为什么要用卷积的原因所在。 三、人类视觉原理在谈及卷积在深度学习的应用之前，我想先分享一下深度学习神经网络是怎么出现的，这是摘自Alex Cai的博客http://www.cnblogs.com/alexcai/p/5506806.html它更能帮助我们了解神经网络的作用，以及卷积在其中扮演的角色。 深度学习的许多研究成果，离不开对大脑认知原理的研究，尤其是视觉原理的研究。 1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是“发现了视觉系统的信息处理”，可视皮层是分级的。 人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。下面是人脑进行人脸识别的一个示例： 对于不同的物体，人类视觉也是通过这样逐层分级，来进行认知的： 我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。 那么我们可以很自然的想到：可以不可以模仿人类大脑的这个特点，构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类呢？答案是肯定的，这也是许多深度学习算法（包括CNN）的灵感来源。 四、深度学习中的卷积由上一节可见，机器学习脱胎于人类视觉原理，而卷积的作用，就是把关键细节抽象化，因为同一类事物总是有各种相同或者相似的细节，记得卷积的作用吗？加权叠加可以提取出相同的、我们所需要的细节并强调它，一层层的提取和强调，就可以生成机器识物的“逻辑”。 在深度学习中，我们所使用的卷积是二维的，因为机器所“看”的图片实质上是一个矩阵，那么相应的，卷积核也是一个矩阵，这个卷积核在 2 维输入数据上“滑动”，对当前输入的部分元素进行矩阵乘法，然后将结果汇为单个输出像素。 这里曾是让我最百思不得其解的地方，可能是智商限制了我的发挥，很长时间有一个问题困扰着我：一维卷积和二维卷积有什么关系？为什么二维卷积会这么计算而不是别的什么方法？其实稍微转换一下角度就很好理解：把输入图像的矩阵和卷积核排成一行而不是将它以二维的形式放置，就会发现它们其实就是f(x)和g(x)以离散的形式所构成的函数！实质上图像正是以这样一种方式，通过训练得出的卷积核，来抽象出图像的特征：强调个性，抑制共性，抽丝剥茧出一套“逻辑”来。 五、后记以上是我在一天的学习后所做的总结，本文没有涉及任何高级的操作，只是我对卷积的一些浅显认识过程。我认为，在深度学习中，打好基础，掌握必要概念非常重要，它会对后面的学习起到意想不到的效果！希望能给予想要学习深度学习的读者一个比较清楚的认识！ 参考链接：如何通俗易懂地解释卷积？ - 张俊博的回答 - 知乎https://www.zhihu.com/question/22298352/answer/34267457卷积为什么叫「卷」积？ - 荆哲的回答 - 知乎https://www.zhihu.com/question/54677157/answer/141245297Alex Cai的博客http://www.cnblogs.com/alexcai/p/5506806.html以及所有我看过的博客们！转载请注明出处]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[图像初级操作]]></title>
    <url>%2F2019%2F03%2F10%2FOpenCV-Learning-Day-1%2F</url>
    <content type="text"><![CDATA[本文内容包括一、erode 图像腐蚀（dilate 图像膨胀）二、blur 均值滤波三、canny边缘检测 文末提供测试图片 一、erode 图像腐蚀（dilate 图像膨胀）1234567891011121314151617#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123; Mat srcImage = imread("Phone.png"); imshow("former",srcImage); Mat element = getStructuringElement(MORPH_RECT,Size(15,15)); Mat dstImage; erode(srcImage,dstImage,element); imshow("later",dstImage); waitKey(0); return 0;&#125; 1.imread函数只能读取绝对路径，不能读取相对路径，存在疑问。已解决：cmake编译时出现问题，需在CMakeLists.txt中加入一行1aux_source_directory(. DIR_SRCS)对整个文件夹进行扫描即可读取到图片的相对路径。（Xcode肯定没这问题但是我不想用哈哈～谁叫它长得没有VS Code好看） 2.getStructuringElement函数原型：1Mat getStructuringElement(int shape, Size esize, Point anchor = Point(-1, -1));函数的第一个参数表示内核的形状，有三种形状可以选择。矩形：MORPH_RECT;交叉形：MORPH_CORSS;椭圆形：MORPH_ELLIPSE;第二和第三个参数分别是内核的尺寸以及锚点的位置。一般在调用erode以及dilate函数之前，先定义一个Mat类型的变量来获得getStructuringElement函数的返回值。对于锚点的位置，有默认值Point（-1,-1），表示锚点位于中心点。element形状唯一依赖锚点位置，其他情况下，锚点只是影响了形态学运算结果的偏移。 3.erode 图像腐蚀（dilate 图像膨胀）erode 函数原型：12void erode( const Mat&amp; src, Mat&amp; dst, const Mat&amp; element,Point anchor=Point(-1,-1), int iterations=1,int borderType=BORDER_CONSTANT,const Scalar&amp; borderValue=morphologyDefaultBorderValue() );dilate 函数原型：12void dilate( const Mat&amp; src, Mat&amp; dst, const Mat&amp; element,Point anchor=Point(-1,-1), int iterations=1,int borderType=BORDER_CONSTANT,const Scalar&amp; borderValue=morphologyDefaultBorderValue() );参数：src:原图像。dst：目标图像。element:腐蚀操作的内核。 如果不指定，默认为一个简单的 3x3 矩阵。否则，我们就要明确指定它的形状，可以使用函数getStructuringElement().anchor:默认为Point(-1,-1),内核中心点。省略时为默认值。iterations:腐蚀次数。省略时为默认值1。borderType:推断边缘类型，具体参见borderInterpolate函数。默认为BORDER_DEFAULT，省略时为默认值。borderValue:边缘值，具体可参见createMorphoogyFilter函数。可省略。 二、blur 均值滤波1234567891011121314#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123; Mat srcImage = imread("Phone.png"); imshow("src",srcImage); Mat dstImage; blur(srcImage,dstImage,Size(7,7)); imshow("dst",dstImage); waitKey(0);&#125; 均值滤波是一种典型的线性滤波算法，主要是利用像素点邻域的像素值来计算像素点的值。其具体方法是首先给出一个滤波模板kernel，该模板将覆盖像素点周围的其他邻域像素点，去掉像素本身，将其邻域像素点相加然后取平均值即为该像素点的新的像素值，这就是均值滤波的本质。函数原型：1void blur(InputArray src, OutputArray dst, Size ksize, Point anchor=Point(-1,-1), int borderType=BORDER_DEFAULT);参数解释：InputArray src: 输入图像OutputArray dst: 输出图像Size ksize: 滤波模板kernel的尺寸，如Size(3,3)Point anchor=Point(-1, -1): 字面意思是锚点，也就是处理的像素位于kernel的什么位置，默认值为(-1, -1)即位于kernel中心点，如果没有特殊需要则不需要更改int borderType=BORDER_DEFAULT: 用于推断图像外部像素的某种边界模式，有默认值BORDER_DEFAULT 三、canny边缘检测12345678910111213141516171819202122232425262728#include&lt;opencv2/opencv.hpp&gt;#include&lt;opencv2/highgui/highgui.hpp&gt;#include&lt;opencv2/imgproc/imgproc.hpp&gt;#include&lt;string&gt;using namespace cv;using namespace std;int main()&#123; Mat srcImage = imread("Phone.png"); //原图像 imshow("src",srcImage); Mat dstImage,edge,grayImage; //创建与srcImage同大小和同类型的矩阵 dstImage.create(srcImage.size(),srcImage.type()); //转换成灰度图像 cvtColor(srcImage,grayImage,COLOR_BGR2GRAY); //使用滤波函数降噪，这里用的是10*10内核 blur(grayImage,edge,Size(10,10)); //运行canny算子 Canny(edge,edge,3,9,3); //目标图像 imshow("dst",edge); waitKey(0);&#125; 1.cvtColor颜色空间转换函数函数原型：123456void cvtColor( InputArray src, // 输入图像 OutputArray dst, // 输出图像 int code, // 颜色映射码 int dstCn = 0 // 输出的通道数 (0='automatic') )cvtColor()支持多种颜色空间之间的转换，目前常见的颜色空间均支持，并且在转换的过程中能够保证数据的类型不变，即转换后的图像的数据类型和位深与源图像一致。 2.canny算法Canny边缘检测于1986年由JOHN CANNY首次在论文《A Computational Approach to Edge Detection》中提出，就此拉开了Canny边缘检测算法的序幕。Canny边缘检测是从不同视觉对象中提取有用的结构信息并大大减少要处理的数据量的一种技术，目前已广泛应用于各种计算机视觉系统。函数原型：123456789101112void cv::Canny ( InputArray image, //输入图像，必须为单通道灰度图 OutputArray edges, //输出图像，为单通道黑白图 double threshold1, double threshold2, //第三个参数和第四个参数表示阈值，这二个阈值中当中的小阈值用来控制边缘连接，大的阈值用来控制强边缘的初始分割，即如果一个像素的梯度大于上限值，则被认为是边缘像素，如果小于下限阈值，则被抛弃。如果该点的梯度在两者之间则当这个点与高于上限值的像素点连接时我们才保留，否则删除。 int apertureSize = 3, //第五个参数表示Sobel 算子大小，默认为3即表示一个3*3的矩阵。Sobel 算子与高斯拉普拉斯算子都是常用的边缘算子 bool L2gradient = false ) 阈值的大小比最好为2：1或3：1。 测试图像：]]></content>
      <categories>
        <category>OpenCV</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mac 使用VS Code 通过cmake 配置 OpenCV和Pytorch C++ API]]></title>
    <url>%2F2019%2F03%2F05%2FDeep-Learning-Starting%2F</url>
    <content type="text"><![CDATA[前情由于pytorch的1.0正式版发布不久，同时较为稳定的c++的API也是在正式版中提供支持，网上的教程不多，因此可供参考的资料只有官方文档和零零散散的博客。不过由于Mac和Linux本身相差不大，一些Linux的配置教程同样值得参考。以下给出链接：官方文档https://pytorch.org/cppdocs/installing.htmlOldpan的个人博客（特别感谢Oldpan老哥的帖子给我的巨大帮助）：利用Pytorch的C++前端(libtorch)读取预训练权重并进行预测：https://oldpan.me/archives/pytorch-c-libtorch-inferencePytorch源码编译简明指南：https://m.oldpan.me/archives/pytorch-build-simple-instruction 开始所需安装的工具有：1.VS Code：这个官网下载即可2.OpenCV 4.0.1:终端输入1brew install opencv即可安装3.Pytorch1.0:可参考上面给出的Pytorch源码编译简明指南，首先在终端输入1git clone --recursive https://github.com/pytorch/pytorch获取最新源码，然后通过编译得到Mac可以读取的.dylib文件（注意：在官方文档中下载的libtorch-shared-with-deps-latest.zip文件解压后所得到的文件夹里的动态库文件是以.so结尾，是Linux下的动态库文件，Mac识别不了）编译时应该先进入到刚刚下载好的Pytorch文件夹（默认的路径应该是/Users/用户名/pytorch）下，然后终端执行123mkdir buildcd buildpython ../tools/build_libtorch.py进行libtorch（即c++ API）的编译，时间较长。4.编译好之后打开VS Code新建一个工程，在这里我引用Oldpan老哥的例子 工程名叫simnet，然后在simnet文件夹下新建一个CMakeLists.txt和一个test.cpp（build先不建），CMakeLists.txt中的代码为：123456789101112131415161718192021cmake_minimum_required(VERSION 3.12 FATAL_ERROR)project(simnet)find_package(Torch REQUIRED) # 查找libtorchfind_package(OpenCV REQUIRED) # 查找OpenCVif(NOT Torch_FOUND)message(FATAL_ERROR "Pytorch Not Found!")endif(NOT Torch_FOUND)message(STATUS "Pytorch status:")message(STATUS " libraries: $&#123;TORCH_LIBRARIES&#125;")message(STATUS "OpenCV library status:")message(STATUS " version: $&#123;OpenCV_VERSION&#125;")message(STATUS " libraries: $&#123;OpenCV_LIBS&#125;")message(STATUS " include path: $&#123;OpenCV_INCLUDE_DIRS&#125;")add_executable(simnet test.cpp)target_link_libraries(simnet $&#123;TORCH_LIBRARIES&#125; $&#123;OpenCV_LIBS&#125;) set_property(TARGET simnet PROPERTY CXX_STANDARD 11) test.cpp中的代码为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#include &lt;opencv2/opencv.hpp&gt;#include "torch/script.h"#include "torch/torch.h"#include &lt;iostream&gt;#include &lt;memory&gt;using namespace std;// resize并保持图像比例不变cv::Mat resize_with_ratio(cv::Mat&amp; img) &#123; cv::Mat temImage; int w = img.cols; int h = img.rows; float t = 1.; float len = t * std::max(w, h); int dst_w = 224, dst_h = 224; cv::Mat image = cv::Mat(cv::Size(dst_w, dst_h), CV_8UC3, cv::Scalar(128,128,128)); cv::Mat imageROI; if(len==w) &#123; float ratio = (float)h/(float)w; cv::resize(img,temImage,cv::Size(224,224*ratio),0,0,cv::INTER_LINEAR); imageROI = image(cv::Rect(0, ((dst_h-224*ratio)/2), temImage.cols, temImage.rows)); temImage.copyTo(imageROI); &#125; else &#123; float ratio = (float)w/(float)h; cv::resize(img,temImage,cv::Size(224*ratio,224),0,0,cv::INTER_LINEAR); imageROI = image(cv::Rect(((dst_w-224*ratio)/2), 0, temImage.cols, temImage.rows)); temImage.copyTo(imageROI); &#125; return image;&#125;int main(int argc, const char* argv[])&#123; if (argc != 2) &#123; std::cerr &lt;&lt; "usage: example-app &lt;path-to-exported-script-module&gt;\n"; return -1; &#125; cv::VideoCapture stream(0); cv::namedWindow("Gesture Detect", cv::WINDOW_AUTOSIZE); std::shared_ptr&lt;torch::jit::script::Module&gt; module = torch::jit::load(argv[1]); module-&gt;to(at::kCUDA); cv::Mat frame; cv::Mat image; cv::Mat input; while(1) &#123; stream&gt;&gt;frame; image = resize_with_ratio(frame); imshow("resized image",image); //显示摄像头的数据 cv::cvtColor(image, input, cv::COLOR_BGR2RGB); // 下方的代码即将图像转化为Tensor，随后导入模型进行预测 torch::Tensor tensor_image = torch::from_blob(input.data, &#123;1,input.rows, input.cols,3&#125;, torch::kByte); tensor_image = tensor_image.permute(&#123;0,3,1,2&#125;); tensor_image = tensor_image.toType(torch::kFloat); tensor_image = tensor_image.div(255); tensor_image = tensor_image.to(torch::kCUDA); torch::Tensor result = module-&gt;forward(&#123;tensor_image&#125;).toTensor(); auto max_result = result.max(1, true); auto max_index = std::get&lt;1&gt;(max_result).item&lt;float&gt;(); if(max_index == 0) cv::putText(frame, "paper", &#123;40, 50&#125;, cv::FONT_HERSHEY_PLAIN, 2.0, cv::Scalar(0, 255, 0), 2); else if(max_index == 1) cv::putText(frame, "scissors", &#123;40, 50&#125;, cv::FONT_HERSHEY_PLAIN, 2.0, cv::Scalar(0, 255, 0), 2); else cv::putText(frame, "stone", &#123;40, 50&#125;, cv::FONT_HERSHEY_PLAIN, 2.0, cv::Scalar(0, 255, 0), 2); imshow("Gesture Detect",frame); //显示摄像头的数据 cv::waitKey(30);&#125; 保存。 5.终端cd进入simnet工程文件夹，然后执行1234mkdir buildcd buildcmake -DCMAKE_PREFIX_PATH=/absolute/path/to/pytorch ..make其中/absolute/path/to/pytorch是pytorch文件夹的绝对路径，一般是/Users/用户名/pytorch这样就编译完成了可以执行1./simnet来运行你的工程了！ 后记由于这是我第一次写教程，很多地方可能有所疏漏，并且配置的过程中踩了无数的坑，可能很多地方起了效果但是我根本没有注意到，还有前期的一些准备工作我也没有提及（比如说anaconda的安装，pip、conda、brew的安装和更新），这些网上有很多大佬写的非常详尽的教程，大家可以多多参考，我这里只是提供了一个自己的思路，如果没有安装成功还望见谅！]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《La La Land》：有关闪耀而易碎的、遗憾而完美的那些事]]></title>
    <url>%2F2019%2F01%2F21%2FLand%2F</url>
    <content type="text"><![CDATA[City of stars, are you shining for me? LA，los angeles，city of stars，确实太过闪耀，有太多人，哪怕从未踏上洛杉矶的土地，也对那里根种了太多美好的幻想。我想起几年前我拿着我新买的PS4，打开《GTA V》的时候，我的天！主角Michael的家外边就是粼粼的海面和充满“热情”的沙滩，无云而湛蓝的天际延伸至远处渐渐变淡，与海面交融直至分不清楚。坐上红色跑车，手搭在车窗上，沿着海岸线一路向北，深红色的岩石蒸腾着热气，视线延伸向上看见绿色————是棕榈树————阳光刺眼，模模糊糊，看不真切；渐入钢铁丛林，阳光也是热辣的吓人，大片大片的反光，大片大片的明媚，西装革履的男人，潮流青年，穿休闲服遛狗的人，酒鬼，或昂首挺胸，或散散漫漫，或失魂落魄，穿梭在层层叠叠的阴影和光斑之间，完美融进这这城市里，一如他们迥然的处境和各不相同的命运。也许是Rockstar太过顶级的美工，又有可能是他们早年的作品圣安第列斯已经满足了我的杀戮欲望，现在我规规矩矩地在马路上开着我的车，老老实实的等着红灯熄灭，绿灯闪烁，循规蹈矩的和其他市民一般驾驶。再往上开，房子渐渐的又变矮了，地势升高，一块巨大的标牌从山顶慢慢向我挪来。我慢慢驶近，在不经意间，它突然变得明亮起来。 啊，HOLLY WOOD。 闪亮的灯牌提醒我天色已晚。我下了车，站在富人区的山顶上，眺望着整个圣洛都。细小闪耀的灯光如同沙粒，连着川流不息的车流，成了江成了河，向我诉说这座钢之巨人自他从这片泥土地里诞生起便永不入睡。是啊，它是如此勤奋地活着，连同它身体里的人们，仿佛都是永不入睡的，为了梦想，为了家庭，为了生活，抑或是吸了毒嗨了药，who cares？圣洛都的天空，是我见过的最美的天空————渐变色的，难以形容————或者可以想象一下，你现在正在一个晚宴上，和你心仪的人交谈，总有那么一两个时候你会觉得羞涩，心里有头小鹿撞来撞去，眼神不知道该往哪里看；于是你只好低头盯着你手里拿着的酒杯，因为这样显得你优雅而有礼貌，你开始端详葡萄酒的颜色：深红的葡萄酒被晚宴上昏暗的光线一打，自底向上由浅入深，不断的变换着色彩，可能是浅粉色，可能是淡蓝色，可能是深紫色；你挪动酒杯，颜色也跟着变化————是了，这就是LA的夜空，不仅仅是渐变的，柔和的，绚丽的，同时，它也是暧昧的，易碎的，不可告人的。 后来我打通了游戏，我最后一次看着Michael，Franklin，Trevor站在一起，看着车子渐渐沉进海底，三个人各自说着像总结一样的话，仿佛是隔着屏幕告诉我们他们的故事到这里就结束了，从今往后他们三个再在哪里相见，以什么样的方式相见，都不再关我的事，自北扬克顿拉开帷幕的故事到这里收场，生活还要继续；后来，因为学习，我（被强迫）收起了我的PS4，到那个时候我已经能轻车熟路的开遍圣洛都的大街小巷，我已然成为了一个老圣洛都人；可是那一晚，我打开我新买的PS4，打开我新买的《GTA V》，逛了逛海滩，坐上我的红色跑车，一路向北……那一幕幕带给我的震撼，我将永远不会忘记。因为是从那个时候起，我真正地将游戏打心底里视为一种艺术，而不是什么害人的消遣，到后来，我从网上知道原来GTA系列是描述美国梦的，我总是不由自主地和那个夜晚联系在一起——繁华的，易碎的，暧昧的，不可告人的……大概就是这样的吧。 我写了这么长的铺垫，其实是想告诉你，为什么我会对这部电影感触如此之深：不仅仅是因为我再次看到了那些似曾相识的场景，美丽的难以忘怀；而且我还看到了另一个故事，有关美国梦的故事，美好的令人心碎。一如这部电影的画面，我明知现实生活中不可能发生这样的故事，却根本不愿意去质疑。就让我带着迷蒙的的眼神，盯着那只葡萄酒杯；就让我微醺的脸色，更浓一分吧。 就当我做了一个梦，而我选择再不复醒。 还没写完先放放。]]></content>
      <categories>
        <category>Film Comment</category>
      </categories>
  </entry>
</search>
